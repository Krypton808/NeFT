[2023-12-06 08:08:20,057] torch.distributed.run: [WARNING] 
[2023-12-06 08:08:20,057] torch.distributed.run: [WARNING] *****************************************
[2023-12-06 08:08:20,057] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2023-12-06 08:08:20,057] torch.distributed.run: [WARNING] *****************************************
[2023-12-06 08:08:23,179] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-12-06 08:08:23,187] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-12-06 08:08:23,188] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
['wandb']
['wandb']
['wandb']
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.91s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.55s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.45s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.64s/it]
/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
model.embed_tokens.weight
/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
model.layers.0.self_attn.q_proj.weight
model.layers.0.self_attn.k_proj.weight
model.layers.0.self_attn.v_proj.weight
model.layers.0.self_attn.o_proj.weight
/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
model.layers.0.mlp.gate_proj.weight
model.layers.0.mlp.up_proj.weight
model.layers.0.mlp.down_proj.weight
model.layers.0.input_layernorm.weight
model.layers.0.post_attention_layernorm.weight
model.layers.1.self_attn.q_proj.weight
model.layers.1.self_attn.k_proj.weight
model.layers.1.self_attn.v_proj.weight
model.layers.1.self_attn.o_proj.weight
model.layers.1.mlp.gate_proj.weight
model.layers.1.mlp.up_proj.weight
model.layers.1.mlp.down_proj.weight
model.layers.1.input_layernorm.weight
model.layers.1.post_attention_layernorm.weight
model.layers.2.self_attn.q_proj.weight
model.layers.2.self_attn.k_proj.weight
model.layers.2.self_attn.v_proj.weight
model.layers.2.self_attn.o_proj.weight
model.layers.2.mlp.gate_proj.weight
model.layers.2.mlp.up_proj.weight
model.layers.2.mlp.down_proj.weight
model.layers.2.input_layernorm.weight
model.layers.2.post_attention_layernorm.weight
model.layers.3.self_attn.q_proj.weight
model.layers.3.self_attn.k_proj.weight
model.layers.3.self_attn.v_proj.weight
model.layers.3.self_attn.o_proj.weight
model.layers.3.mlp.gate_proj.weight
model.layers.3.mlp.up_proj.weight
model.layers.3.mlp.down_proj.weight
model.layers.3.input_layernorm.weight
model.layers.3.post_attention_layernorm.weight
model.layers.4.self_attn.q_proj.weight
model.layers.4.self_attn.k_proj.weight
model.layers.4.self_attn.v_proj.weight
model.layers.4.self_attn.o_proj.weight
model.layers.4.mlp.gate_proj.weight
model.layers.4.mlp.up_proj.weight
model.layers.4.mlp.down_proj.weight
model.layers.4.input_layernorm.weight
model.layers.4.post_attention_layernorm.weight
model.layers.5.self_attn.q_proj.weight
model.layers.5.self_attn.k_proj.weight
model.layers.5.self_attn.v_proj.weight
model.layers.5.self_attn.o_proj.weight
model.layers.5.mlp.gate_proj.weight
model.layers.5.mlp.up_proj.weight
model.layers.5.mlp.down_proj.weight
model.layers.5.input_layernorm.weight
model.layers.5.post_attention_layernorm.weight
model.layers.6.self_attn.q_proj.weight
model.layers.6.self_attn.k_proj.weight
model.layers.6.self_attn.v_proj.weight
model.layers.6.self_attn.o_proj.weight
model.layers.6.mlp.gate_proj.weight
model.layers.6.mlp.up_proj.weight
model.layers.6.mlp.down_proj.weight
model.layers.6.input_layernorm.weight
model.layers.6.post_attention_layernorm.weight
model.layers.7.self_attn.q_proj.weight
model.layers.7.self_attn.k_proj.weight
model.embed_tokens.weightmodel.layers.7.self_attn.v_proj.weight

model.layers.7.self_attn.o_proj.weight
model.layers.0.self_attn.q_proj.weightmodel.layers.7.mlp.gate_proj.weight

model.layers.0.self_attn.k_proj.weightmodel.layers.7.mlp.up_proj.weight

model.layers.0.self_attn.v_proj.weightmodel.layers.7.mlp.down_proj.weight

model.layers.0.self_attn.o_proj.weight
model.layers.7.input_layernorm.weight
model.layers.7.post_attention_layernorm.weight
model.layers.0.mlp.gate_proj.weight
model.layers.8.self_attn.q_proj.weightmodel.layers.0.mlp.up_proj.weight

model.layers.8.self_attn.k_proj.weightmodel.layers.0.mlp.down_proj.weight

model.layers.8.self_attn.v_proj.weight
model.layers.0.input_layernorm.weight
model.layers.8.self_attn.o_proj.weightmodel.layers.0.post_attention_layernorm.weight

model.layers.8.mlp.gate_proj.weight
model.layers.1.self_attn.q_proj.weight
model.layers.8.mlp.up_proj.weight
model.layers.1.self_attn.k_proj.weight
model.layers.8.mlp.down_proj.weight
model.layers.1.self_attn.v_proj.weight
model.layers.8.input_layernorm.weightmodel.layers.1.self_attn.o_proj.weight

model.layers.8.post_attention_layernorm.weight
model.layers.1.mlp.gate_proj.weight
model.layers.9.self_attn.q_proj.weightmodel.layers.1.mlp.up_proj.weight

model.layers.9.self_attn.k_proj.weightmodel.layers.1.mlp.down_proj.weight

model.layers.9.self_attn.v_proj.weight
model.layers.1.input_layernorm.weight
model.layers.9.self_attn.o_proj.weight
model.layers.1.post_attention_layernorm.weight
model.layers.9.mlp.gate_proj.weight
model.layers.2.self_attn.q_proj.weight
model.layers.9.mlp.up_proj.weight
model.layers.2.self_attn.k_proj.weight
model.layers.9.mlp.down_proj.weight
model.layers.2.self_attn.v_proj.weight
model.layers.9.input_layernorm.weightmodel.layers.2.self_attn.o_proj.weight

model.layers.9.post_attention_layernorm.weight
model.layers.2.mlp.gate_proj.weight
model.layers.10.self_attn.q_proj.weightmodel.layers.2.mlp.up_proj.weight

model.layers.10.self_attn.k_proj.weightmodel.layers.2.mlp.down_proj.weight

model.layers.10.self_attn.v_proj.weight
model.layers.2.input_layernorm.weight
model.layers.10.self_attn.o_proj.weight
model.layers.2.post_attention_layernorm.weight
model.layers.10.mlp.gate_proj.weight
model.layers.3.self_attn.q_proj.weight
model.layers.10.mlp.up_proj.weight
model.layers.3.self_attn.k_proj.weight
model.layers.10.mlp.down_proj.weight
model.layers.3.self_attn.v_proj.weight
model.layers.10.input_layernorm.weightmodel.layers.3.self_attn.o_proj.weight

model.layers.10.post_attention_layernorm.weight
model.layers.3.mlp.gate_proj.weight
model.layers.11.self_attn.q_proj.weightmodel.layers.3.mlp.up_proj.weight

model.layers.11.self_attn.k_proj.weightmodel.layers.3.mlp.down_proj.weight

model.layers.11.self_attn.v_proj.weight
model.layers.3.input_layernorm.weight
model.layers.11.self_attn.o_proj.weight
model.layers.3.post_attention_layernorm.weight
model.layers.11.mlp.gate_proj.weight
model.layers.4.self_attn.q_proj.weight
model.layers.11.mlp.up_proj.weight
model.layers.4.self_attn.k_proj.weight
model.layers.11.mlp.down_proj.weight
model.layers.4.self_attn.v_proj.weight
model.layers.11.input_layernorm.weightmodel.layers.4.self_attn.o_proj.weight

model.layers.11.post_attention_layernorm.weight
model.layers.4.mlp.gate_proj.weight
model.layers.12.self_attn.q_proj.weightmodel.layers.4.mlp.up_proj.weight

model.layers.12.self_attn.k_proj.weightmodel.layers.4.mlp.down_proj.weight

model.layers.12.self_attn.v_proj.weight
model.layers.4.input_layernorm.weight
model.layers.12.self_attn.o_proj.weight
model.layers.4.post_attention_layernorm.weight
model.layers.12.mlp.gate_proj.weight
model.layers.5.self_attn.q_proj.weightmodel.layers.12.mlp.up_proj.weight

model.layers.5.self_attn.k_proj.weightmodel.layers.12.mlp.down_proj.weight

model.layers.5.self_attn.v_proj.weight
model.layers.12.input_layernorm.weight
model.layers.5.self_attn.o_proj.weight
model.layers.12.post_attention_layernorm.weight
model.layers.5.mlp.gate_proj.weight
model.layers.13.self_attn.q_proj.weight
model.layers.5.mlp.up_proj.weight
model.layers.13.self_attn.k_proj.weight
model.layers.5.mlp.down_proj.weight
model.layers.13.self_attn.v_proj.weight
model.layers.5.input_layernorm.weightmodel.layers.13.self_attn.o_proj.weight

model.layers.5.post_attention_layernorm.weight
model.layers.13.mlp.gate_proj.weight
model.layers.6.self_attn.q_proj.weightmodel.layers.13.mlp.up_proj.weight

model.layers.6.self_attn.k_proj.weightmodel.layers.13.mlp.down_proj.weight

model.layers.6.self_attn.v_proj.weight
model.layers.13.input_layernorm.weight
model.layers.6.self_attn.o_proj.weight
model.layers.13.post_attention_layernorm.weight
model.layers.6.mlp.gate_proj.weight
model.layers.14.self_attn.q_proj.weight
model.layers.6.mlp.up_proj.weight
model.layers.14.self_attn.k_proj.weight
model.layers.6.mlp.down_proj.weight
model.layers.14.self_attn.v_proj.weight
model.layers.6.input_layernorm.weightmodel.layers.14.self_attn.o_proj.weight

model.layers.6.post_attention_layernorm.weight
model.layers.14.mlp.gate_proj.weight
model.layers.7.self_attn.q_proj.weight
model.layers.14.mlp.up_proj.weight
model.layers.7.self_attn.k_proj.weight
model.layers.14.mlp.down_proj.weight
model.layers.7.self_attn.v_proj.weight
model.layers.14.input_layernorm.weightmodel.layers.7.self_attn.o_proj.weight

model.layers.14.post_attention_layernorm.weight
model.layers.7.mlp.gate_proj.weight
model.layers.15.self_attn.q_proj.weightmodel.layers.7.mlp.up_proj.weight

model.layers.15.self_attn.k_proj.weightmodel.layers.7.mlp.down_proj.weight

model.layers.15.self_attn.v_proj.weight
model.layers.7.input_layernorm.weight
model.layers.15.self_attn.o_proj.weight
model.layers.7.post_attention_layernorm.weight
model.layers.15.mlp.gate_proj.weightmodel.layers.8.self_attn.q_proj.weight

model.layers.15.mlp.up_proj.weightmodel.layers.8.self_attn.k_proj.weight

model.layers.15.mlp.down_proj.weightmodel.layers.8.self_attn.v_proj.weight

model.layers.15.input_layernorm.weightmodel.layers.8.self_attn.o_proj.weight

model.layers.15.post_attention_layernorm.weight
model.layers.8.mlp.gate_proj.weight
model.layers.16.self_attn.q_proj.weightmodel.layers.8.mlp.up_proj.weight

model.layers.16.self_attn.k_proj.weightmodel.layers.8.mlp.down_proj.weight

model.layers.16.self_attn.v_proj.weight
model.layers.8.input_layernorm.weight
model.layers.16.self_attn.o_proj.weight
model.layers.8.post_attention_layernorm.weight
model.layers.16.mlp.gate_proj.weightmodel.layers.9.self_attn.q_proj.weight

model.layers.16.mlp.up_proj.weightmodel.layers.9.self_attn.k_proj.weight

model.layers.16.mlp.down_proj.weightmodel.layers.9.self_attn.v_proj.weight

model.layers.9.self_attn.o_proj.weightmodel.layers.16.input_layernorm.weight

model.layers.16.post_attention_layernorm.weightmodel.layers.9.mlp.gate_proj.weight

model.layers.9.mlp.up_proj.weight
model.layers.17.self_attn.q_proj.weight
model.layers.9.mlp.down_proj.weight
model.layers.17.self_attn.k_proj.weight
model.layers.9.input_layernorm.weight
model.layers.17.self_attn.v_proj.weight
model.layers.9.post_attention_layernorm.weight
model.layers.17.self_attn.o_proj.weight
model.layers.10.self_attn.q_proj.weight
model.layers.17.mlp.gate_proj.weight
model.layers.10.self_attn.k_proj.weight
model.layers.17.mlp.up_proj.weight
model.layers.10.self_attn.v_proj.weight
model.layers.17.mlp.down_proj.weight
model.layers.10.self_attn.o_proj.weight
model.layers.17.input_layernorm.weight
model.layers.10.mlp.gate_proj.weight
model.layers.17.post_attention_layernorm.weight
model.layers.10.mlp.up_proj.weight
model.layers.18.self_attn.q_proj.weightmodel.layers.10.mlp.down_proj.weight

model.layers.18.self_attn.k_proj.weight
model.layers.10.input_layernorm.weight
model.layers.18.self_attn.v_proj.weight
model.layers.10.post_attention_layernorm.weight
model.layers.18.self_attn.o_proj.weight
model.layers.11.self_attn.q_proj.weight
model.layers.18.mlp.gate_proj.weightmodel.layers.11.self_attn.k_proj.weight

model.layers.18.mlp.up_proj.weightmodel.layers.11.self_attn.v_proj.weight

model.layers.18.mlp.down_proj.weightmodel.layers.11.self_attn.o_proj.weight

model.layers.18.input_layernorm.weight
model.layers.11.mlp.gate_proj.weight
model.layers.18.post_attention_layernorm.weight
model.layers.11.mlp.up_proj.weight
model.layers.11.mlp.down_proj.weightmodel.layers.19.self_attn.q_proj.weight

model.layers.11.input_layernorm.weightmodel.layers.19.self_attn.k_proj.weight

model.layers.11.post_attention_layernorm.weightmodel.layers.19.self_attn.v_proj.weight

model.layers.19.self_attn.o_proj.weightmodel.layers.12.self_attn.q_proj.weight

model.layers.12.self_attn.k_proj.weight
model.layers.19.mlp.gate_proj.weight
model.layers.12.self_attn.v_proj.weight
model.layers.19.mlp.up_proj.weight
model.layers.12.self_attn.o_proj.weight
model.layers.19.mlp.down_proj.weight
model.layers.12.mlp.gate_proj.weight
model.layers.19.input_layernorm.weight
model.layers.12.mlp.up_proj.weight
model.layers.19.post_attention_layernorm.weight
model.layers.12.mlp.down_proj.weight
model.layers.20.self_attn.q_proj.weightmodel.layers.12.input_layernorm.weight

model.layers.20.self_attn.k_proj.weightmodel.layers.12.post_attention_layernorm.weight

model.layers.20.self_attn.v_proj.weight
model.layers.13.self_attn.q_proj.weight
model.layers.20.self_attn.o_proj.weightmodel.layers.13.self_attn.k_proj.weight

model.layers.13.self_attn.v_proj.weight
model.layers.20.mlp.gate_proj.weight
model.layers.13.self_attn.o_proj.weight
model.layers.20.mlp.up_proj.weight
model.layers.13.mlp.gate_proj.weightmodel.layers.20.mlp.down_proj.weight

model.layers.13.mlp.up_proj.weight
model.layers.20.input_layernorm.weight
model.layers.13.mlp.down_proj.weight
model.layers.20.post_attention_layernorm.weight
model.layers.13.input_layernorm.weight
model.layers.21.self_attn.q_proj.weightmodel.layers.13.post_attention_layernorm.weight

model.layers.21.self_attn.k_proj.weight
model.layers.14.self_attn.q_proj.weight
model.layers.21.self_attn.v_proj.weight
model.layers.14.self_attn.k_proj.weight
model.layers.21.self_attn.o_proj.weight
model.layers.14.self_attn.v_proj.weight
model.layers.14.self_attn.o_proj.weight
model.layers.21.mlp.gate_proj.weightmodel.layers.14.mlp.gate_proj.weight

model.layers.21.mlp.up_proj.weightmodel.layers.14.mlp.up_proj.weight

model.layers.21.mlp.down_proj.weightmodel.layers.14.mlp.down_proj.weight

model.layers.21.input_layernorm.weightmodel.layers.14.input_layernorm.weight

model.layers.21.post_attention_layernorm.weightmodel.layers.14.post_attention_layernorm.weight

model.layers.22.self_attn.q_proj.weightmodel.layers.15.self_attn.q_proj.weight

model.layers.22.self_attn.k_proj.weightmodel.layers.15.self_attn.k_proj.weight

model.layers.22.self_attn.v_proj.weightmodel.layers.15.self_attn.v_proj.weight

model.layers.22.self_attn.o_proj.weightmodel.layers.15.self_attn.o_proj.weight

model.layers.22.mlp.gate_proj.weightmodel.layers.15.mlp.gate_proj.weight

model.layers.22.mlp.up_proj.weightmodel.layers.15.mlp.up_proj.weight

model.layers.22.mlp.down_proj.weightmodel.layers.15.mlp.down_proj.weight

model.layers.22.input_layernorm.weightmodel.layers.15.input_layernorm.weight

model.layers.22.post_attention_layernorm.weightmodel.layers.15.post_attention_layernorm.weight

model.layers.23.self_attn.q_proj.weightmodel.layers.16.self_attn.q_proj.weight

model.layers.23.self_attn.k_proj.weightmodel.layers.16.self_attn.k_proj.weight

model.layers.23.self_attn.v_proj.weightmodel.layers.16.self_attn.v_proj.weight

model.layers.23.self_attn.o_proj.weightmodel.layers.16.self_attn.o_proj.weight

model.layers.23.mlp.gate_proj.weightmodel.layers.16.mlp.gate_proj.weight

model.layers.23.mlp.up_proj.weightmodel.layers.16.mlp.up_proj.weight

model.layers.23.mlp.down_proj.weightmodel.layers.16.mlp.down_proj.weight

model.layers.23.input_layernorm.weightmodel.layers.16.input_layernorm.weight

model.layers.23.post_attention_layernorm.weightmodel.layers.16.post_attention_layernorm.weight

model.layers.24.self_attn.q_proj.weightmodel.layers.17.self_attn.q_proj.weight

model.layers.24.self_attn.k_proj.weightmodel.layers.17.self_attn.k_proj.weight

model.layers.24.self_attn.v_proj.weightmodel.layers.17.self_attn.v_proj.weight

model.layers.24.self_attn.o_proj.weightmodel.layers.17.self_attn.o_proj.weight

model.layers.24.mlp.gate_proj.weightmodel.layers.17.mlp.gate_proj.weight

model.layers.24.mlp.up_proj.weightmodel.layers.17.mlp.up_proj.weight

model.layers.24.mlp.down_proj.weightmodel.layers.17.mlp.down_proj.weight

model.layers.24.input_layernorm.weightmodel.layers.17.input_layernorm.weight

model.layers.24.post_attention_layernorm.weightmodel.layers.17.post_attention_layernorm.weight

model.layers.25.self_attn.q_proj.weight
model.layers.18.self_attn.q_proj.weight
model.layers.25.self_attn.k_proj.weight
model.layers.18.self_attn.k_proj.weight
model.layers.25.self_attn.v_proj.weight
model.layers.18.self_attn.v_proj.weight
model.layers.25.self_attn.o_proj.weight
model.layers.18.self_attn.o_proj.weight
model.layers.25.mlp.gate_proj.weight
model.layers.18.mlp.gate_proj.weight
model.layers.25.mlp.up_proj.weight
model.layers.18.mlp.up_proj.weight
model.layers.25.mlp.down_proj.weight
model.layers.18.mlp.down_proj.weight
model.layers.25.input_layernorm.weight
model.layers.18.input_layernorm.weight
model.layers.25.post_attention_layernorm.weight
model.layers.18.post_attention_layernorm.weight
model.layers.26.self_attn.q_proj.weightmodel.layers.19.self_attn.q_proj.weight

model.layers.26.self_attn.k_proj.weightmodel.layers.19.self_attn.k_proj.weight

model.layers.26.self_attn.v_proj.weightmodel.layers.19.self_attn.v_proj.weight

model.layers.26.self_attn.o_proj.weightmodel.layers.19.self_attn.o_proj.weight

model.layers.26.mlp.gate_proj.weightmodel.layers.19.mlp.gate_proj.weight

model.layers.26.mlp.up_proj.weightmodel.layers.19.mlp.up_proj.weight

model.layers.26.mlp.down_proj.weightmodel.layers.19.mlp.down_proj.weight

model.layers.26.input_layernorm.weightmodel.layers.19.input_layernorm.weight

model.layers.26.post_attention_layernorm.weightmodel.layers.19.post_attention_layernorm.weight

model.layers.27.self_attn.q_proj.weightmodel.layers.20.self_attn.q_proj.weight

model.layers.27.self_attn.k_proj.weightmodel.layers.20.self_attn.k_proj.weight

model.layers.27.self_attn.v_proj.weightmodel.layers.20.self_attn.v_proj.weight

model.layers.27.self_attn.o_proj.weightmodel.layers.20.self_attn.o_proj.weight

model.layers.27.mlp.gate_proj.weight
model.layers.20.mlp.gate_proj.weight
model.layers.27.mlp.up_proj.weight
model.layers.20.mlp.up_proj.weight
model.layers.27.mlp.down_proj.weight
model.layers.20.mlp.down_proj.weight
model.layers.27.input_layernorm.weight
model.layers.20.input_layernorm.weight
model.layers.27.post_attention_layernorm.weight
model.layers.20.post_attention_layernorm.weight
model.layers.28.self_attn.q_proj.weight
model.layers.21.self_attn.q_proj.weight
model.layers.28.self_attn.k_proj.weight
model.layers.21.self_attn.k_proj.weight
model.layers.28.self_attn.v_proj.weight
model.layers.21.self_attn.v_proj.weight
model.layers.28.self_attn.o_proj.weight
model.layers.21.self_attn.o_proj.weight
model.layers.28.mlp.gate_proj.weight
model.layers.28.mlp.up_proj.weight
model.layers.21.mlp.gate_proj.weight
model.layers.28.mlp.down_proj.weight
model.layers.21.mlp.up_proj.weight
model.layers.28.input_layernorm.weightmodel.layers.21.mlp.down_proj.weight

model.layers.28.post_attention_layernorm.weight
model.layers.21.input_layernorm.weight
model.layers.29.self_attn.q_proj.weightmodel.layers.21.post_attention_layernorm.weight

model.layers.29.self_attn.k_proj.weight
model.layers.22.self_attn.q_proj.weight
model.layers.29.self_attn.v_proj.weight
model.layers.22.self_attn.k_proj.weight
model.layers.29.self_attn.o_proj.weight
model.layers.22.self_attn.v_proj.weight
model.layers.29.mlp.gate_proj.weightmodel.layers.22.self_attn.o_proj.weight

model.layers.29.mlp.up_proj.weight
model.layers.22.mlp.gate_proj.weight
model.layers.29.mlp.down_proj.weight
model.layers.22.mlp.up_proj.weight
model.layers.29.input_layernorm.weightmodel.layers.22.mlp.down_proj.weight

model.layers.29.post_attention_layernorm.weight
model.layers.22.input_layernorm.weight
model.layers.22.post_attention_layernorm.weightmodel.layers.30.self_attn.q_proj.weight

model.layers.30.self_attn.k_proj.weight
model.layers.23.self_attn.q_proj.weight
model.layers.30.self_attn.v_proj.weight
model.layers.23.self_attn.k_proj.weight
model.layers.30.self_attn.o_proj.weight
model.layers.23.self_attn.v_proj.weight
model.layers.30.mlp.gate_proj.weightmodel.layers.23.self_attn.o_proj.weight

model.layers.30.mlp.up_proj.weight
model.layers.23.mlp.gate_proj.weight
model.layers.30.mlp.down_proj.weight
model.layers.23.mlp.up_proj.weight
model.layers.30.input_layernorm.weightmodel.layers.23.mlp.down_proj.weight

model.layers.30.post_attention_layernorm.weight
model.layers.23.input_layernorm.weight
model.layers.23.post_attention_layernorm.weightmodel.layers.31.self_attn.q_proj.weight

model.layers.31.self_attn.k_proj.weightmodel.layers.24.self_attn.q_proj.weight

model.layers.31.self_attn.v_proj.weightmodel.layers.24.self_attn.k_proj.weight

model.layers.31.self_attn.o_proj.weightmodel.layers.24.self_attn.v_proj.weight

model.layers.24.self_attn.o_proj.weight
model.layers.31.mlp.gate_proj.weight
model.layers.24.mlp.gate_proj.weightmodel.layers.31.mlp.up_proj.weight

model.layers.24.mlp.up_proj.weightmodel.layers.31.mlp.down_proj.weight

model.layers.24.mlp.down_proj.weight
model.layers.31.input_layernorm.weightmodel.layers.24.input_layernorm.weight

model.layers.31.post_attention_layernorm.weightmodel.layers.24.post_attention_layernorm.weight

model.norm.weight
model.layers.25.self_attn.q_proj.weight
lm_head.weight
model.layers.25.self_attn.k_proj.weight
model.layers.25.self_attn.v_proj.weight
model.layers.25.self_attn.o_proj.weight
model.layers.25.mlp.gate_proj.weight
model.layers.25.mlp.up_proj.weight
model.layers.25.mlp.down_proj.weight
model.layers.25.input_layernorm.weight
model.layers.25.post_attention_layernorm.weight
model.layers.26.self_attn.q_proj.weight
model.layers.26.self_attn.k_proj.weight
model.layers.26.self_attn.v_proj.weight
model.layers.26.self_attn.o_proj.weight
model.layers.26.mlp.gate_proj.weight
model.layers.26.mlp.up_proj.weight
model.layers.26.mlp.down_proj.weight
model.layers.26.input_layernorm.weight
model.layers.26.post_attention_layernorm.weight
model.layers.27.self_attn.q_proj.weight
model.layers.27.self_attn.k_proj.weight
model.layers.27.self_attn.v_proj.weight
model.layers.27.self_attn.o_proj.weight
model.layers.27.mlp.gate_proj.weight
model.layers.27.mlp.up_proj.weight
model.layers.27.mlp.down_proj.weight
model.layers.27.input_layernorm.weight
model.layers.27.post_attention_layernorm.weight
model.layers.28.self_attn.q_proj.weight
model.layers.28.self_attn.k_proj.weight
model.layers.28.self_attn.v_proj.weight
model.layers.28.self_attn.o_proj.weight
model.layers.28.mlp.gate_proj.weight
model.layers.28.mlp.up_proj.weight
model.layers.28.mlp.down_proj.weight
model.layers.28.input_layernorm.weight
model.layers.28.post_attention_layernorm.weight
model.layers.29.self_attn.q_proj.weight
model.layers.29.self_attn.k_proj.weight
model.layers.29.self_attn.v_proj.weight
model.layers.29.self_attn.o_proj.weight
model.layers.29.mlp.gate_proj.weight
model.layers.29.mlp.up_proj.weight
model.layers.29.mlp.down_proj.weight
model.layers.29.input_layernorm.weight
model.layers.29.post_attention_layernorm.weight
model.layers.30.self_attn.q_proj.weight
model.layers.30.self_attn.k_proj.weight
model.layers.30.self_attn.v_proj.weight
model.layers.30.self_attn.o_proj.weight
model.layers.30.mlp.gate_proj.weight
model.layers.30.mlp.up_proj.weight
model.layers.30.mlp.down_proj.weight
model.layers.30.input_layernorm.weight
model.layers.30.post_attention_layernorm.weight
model.layers.31.self_attn.q_proj.weight
model.layers.31.self_attn.k_proj.weight
model.layers.31.self_attn.v_proj.weight
model.layers.31.self_attn.o_proj.weight
model.layers.31.mlp.gate_proj.weight
model.layers.31.mlp.up_proj.weight
model.layers.31.mlp.down_proj.weight
model.layers.31.input_layernorm.weight
model.layers.31.post_attention_layernorm.weight
model.norm.weight
lm_head.weight
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.58s/it]
/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
model.embed_tokens.weight
model.layers.0.self_attn.q_proj.weight
model.layers.0.self_attn.k_proj.weight
model.layers.0.self_attn.v_proj.weight
model.layers.0.self_attn.o_proj.weight
model.layers.0.mlp.gate_proj.weight
model.layers.0.mlp.up_proj.weight
model.layers.0.mlp.down_proj.weight
model.layers.0.input_layernorm.weight
model.layers.0.post_attention_layernorm.weight
model.layers.1.self_attn.q_proj.weight
model.layers.1.self_attn.k_proj.weight
model.layers.1.self_attn.v_proj.weight
model.layers.1.self_attn.o_proj.weight
model.layers.1.mlp.gate_proj.weight
model.layers.1.mlp.up_proj.weight
model.layers.1.mlp.down_proj.weight
model.layers.1.input_layernorm.weight
model.layers.1.post_attention_layernorm.weight
model.layers.2.self_attn.q_proj.weight
model.layers.2.self_attn.k_proj.weight
model.layers.2.self_attn.v_proj.weight
model.layers.2.self_attn.o_proj.weight
model.layers.2.mlp.gate_proj.weight
model.layers.2.mlp.up_proj.weight
model.layers.2.mlp.down_proj.weight
model.layers.2.input_layernorm.weight
model.layers.2.post_attention_layernorm.weight
model.layers.3.self_attn.q_proj.weight
model.layers.3.self_attn.k_proj.weight
model.layers.3.self_attn.v_proj.weight
model.layers.3.self_attn.o_proj.weight
model.layers.3.mlp.gate_proj.weight
model.layers.3.mlp.up_proj.weight
model.layers.3.mlp.down_proj.weight
model.layers.3.input_layernorm.weight
model.layers.3.post_attention_layernorm.weight
model.layers.4.self_attn.q_proj.weight
model.layers.4.self_attn.k_proj.weight
model.layers.4.self_attn.v_proj.weight
model.layers.4.self_attn.o_proj.weight
model.layers.4.mlp.gate_proj.weight
model.layers.4.mlp.up_proj.weight
model.layers.4.mlp.down_proj.weight
model.layers.4.input_layernorm.weight
model.layers.4.post_attention_layernorm.weight
model.layers.5.self_attn.q_proj.weight
model.layers.5.self_attn.k_proj.weight
model.layers.5.self_attn.v_proj.weight
model.layers.5.self_attn.o_proj.weight
model.layers.5.mlp.gate_proj.weight
model.layers.5.mlp.up_proj.weight
model.layers.5.mlp.down_proj.weight
model.layers.5.input_layernorm.weight
model.layers.5.post_attention_layernorm.weight
model.layers.6.self_attn.q_proj.weight
model.layers.6.self_attn.k_proj.weight
model.layers.6.self_attn.v_proj.weight
model.layers.6.self_attn.o_proj.weight
model.layers.6.mlp.gate_proj.weight
model.layers.6.mlp.up_proj.weight
model.layers.6.mlp.down_proj.weight
model.layers.6.input_layernorm.weight
model.layers.6.post_attention_layernorm.weight
model.layers.7.self_attn.q_proj.weight
model.layers.7.self_attn.k_proj.weight
model.layers.7.self_attn.v_proj.weight
model.layers.7.self_attn.o_proj.weight
model.layers.7.mlp.gate_proj.weight
model.layers.7.mlp.up_proj.weight
model.layers.7.mlp.down_proj.weight
model.layers.7.input_layernorm.weight
model.layers.7.post_attention_layernorm.weight
model.layers.8.self_attn.q_proj.weight
model.layers.8.self_attn.k_proj.weight
model.layers.8.self_attn.v_proj.weight
model.layers.8.self_attn.o_proj.weight
model.layers.8.mlp.gate_proj.weight
model.layers.8.mlp.up_proj.weight
model.layers.8.mlp.down_proj.weight
model.layers.8.input_layernorm.weight
model.layers.8.post_attention_layernorm.weight
model.layers.9.self_attn.q_proj.weight
model.layers.9.self_attn.k_proj.weight
model.layers.9.self_attn.v_proj.weight
model.layers.9.self_attn.o_proj.weight
model.layers.9.mlp.gate_proj.weight
model.layers.9.mlp.up_proj.weight
model.layers.9.mlp.down_proj.weight
model.layers.9.input_layernorm.weight
model.layers.9.post_attention_layernorm.weight
model.layers.10.self_attn.q_proj.weight
model.layers.10.self_attn.k_proj.weight
model.layers.10.self_attn.v_proj.weight
model.layers.10.self_attn.o_proj.weight
model.layers.10.mlp.gate_proj.weight
model.layers.10.mlp.up_proj.weight
model.layers.10.mlp.down_proj.weight
model.layers.10.input_layernorm.weight
model.layers.10.post_attention_layernorm.weight
model.layers.11.self_attn.q_proj.weight
model.layers.11.self_attn.k_proj.weight
model.layers.11.self_attn.v_proj.weight
model.layers.11.self_attn.o_proj.weight
model.layers.11.mlp.gate_proj.weight
model.layers.11.mlp.up_proj.weight
model.layers.11.mlp.down_proj.weight
model.layers.11.input_layernorm.weight
model.layers.11.post_attention_layernorm.weight
model.layers.12.self_attn.q_proj.weight
model.layers.12.self_attn.k_proj.weight
model.layers.12.self_attn.v_proj.weight
model.layers.12.self_attn.o_proj.weight
model.layers.12.mlp.gate_proj.weight
model.layers.12.mlp.up_proj.weight
model.layers.12.mlp.down_proj.weight
model.layers.12.input_layernorm.weight
model.layers.12.post_attention_layernorm.weight
model.layers.13.self_attn.q_proj.weight
model.layers.13.self_attn.k_proj.weight
model.layers.13.self_attn.v_proj.weight
model.layers.13.self_attn.o_proj.weight
model.layers.13.mlp.gate_proj.weight
model.layers.13.mlp.up_proj.weight
model.layers.13.mlp.down_proj.weight
model.layers.13.input_layernorm.weight
model.layers.13.post_attention_layernorm.weight
model.layers.14.self_attn.q_proj.weight
model.layers.14.self_attn.k_proj.weight
model.layers.14.self_attn.v_proj.weight
model.layers.14.self_attn.o_proj.weight
model.layers.14.mlp.gate_proj.weight
model.layers.14.mlp.up_proj.weight
model.layers.14.mlp.down_proj.weight
model.layers.14.input_layernorm.weight
model.layers.14.post_attention_layernorm.weight
model.layers.15.self_attn.q_proj.weight
model.layers.15.self_attn.k_proj.weight
model.layers.15.self_attn.v_proj.weight
model.layers.15.self_attn.o_proj.weight
model.layers.15.mlp.gate_proj.weight
model.layers.15.mlp.up_proj.weight
model.layers.15.mlp.down_proj.weight
model.layers.15.input_layernorm.weight
model.layers.15.post_attention_layernorm.weight
model.layers.16.self_attn.q_proj.weight
model.layers.16.self_attn.k_proj.weight
model.layers.16.self_attn.v_proj.weight
model.layers.16.self_attn.o_proj.weight
model.layers.16.mlp.gate_proj.weight
model.layers.16.mlp.up_proj.weight
model.layers.16.mlp.down_proj.weight
model.layers.16.input_layernorm.weight
model.layers.16.post_attention_layernorm.weight
model.layers.17.self_attn.q_proj.weight
model.layers.17.self_attn.k_proj.weight
model.layers.17.self_attn.v_proj.weight
model.layers.17.self_attn.o_proj.weight
model.layers.17.mlp.gate_proj.weight
model.layers.17.mlp.up_proj.weight
model.layers.17.mlp.down_proj.weight
model.layers.17.input_layernorm.weight
model.layers.17.post_attention_layernorm.weight
model.layers.18.self_attn.q_proj.weight
model.layers.18.self_attn.k_proj.weight
model.layers.18.self_attn.v_proj.weight
model.layers.18.self_attn.o_proj.weight
model.layers.18.mlp.gate_proj.weight
model.layers.18.mlp.up_proj.weight
model.layers.18.mlp.down_proj.weight
model.layers.18.input_layernorm.weight
model.layers.18.post_attention_layernorm.weight
model.layers.19.self_attn.q_proj.weight
model.layers.19.self_attn.k_proj.weight
model.layers.19.self_attn.v_proj.weight
model.layers.19.self_attn.o_proj.weight
model.layers.19.mlp.gate_proj.weight
model.layers.19.mlp.up_proj.weight
model.layers.19.mlp.down_proj.weight
model.layers.19.input_layernorm.weight
model.layers.19.post_attention_layernorm.weight
model.layers.20.self_attn.q_proj.weight
model.layers.20.self_attn.k_proj.weight
model.layers.20.self_attn.v_proj.weight
model.layers.20.self_attn.o_proj.weight
model.layers.20.mlp.gate_proj.weight
model.layers.20.mlp.up_proj.weight
model.layers.20.mlp.down_proj.weight
model.layers.20.input_layernorm.weight
model.layers.20.post_attention_layernorm.weight
model.layers.21.self_attn.q_proj.weight
model.layers.21.self_attn.k_proj.weight
model.layers.21.self_attn.v_proj.weight
model.layers.21.self_attn.o_proj.weight
model.layers.21.mlp.gate_proj.weight
model.layers.21.mlp.up_proj.weight
model.layers.21.mlp.down_proj.weight
model.layers.21.input_layernorm.weight
model.layers.21.post_attention_layernorm.weight
model.layers.22.self_attn.q_proj.weight
model.layers.22.self_attn.k_proj.weight
model.layers.22.self_attn.v_proj.weight
model.layers.22.self_attn.o_proj.weight
model.layers.22.mlp.gate_proj.weight
model.layers.22.mlp.up_proj.weight
model.layers.22.mlp.down_proj.weight
model.layers.22.input_layernorm.weight
model.layers.22.post_attention_layernorm.weight
model.layers.23.self_attn.q_proj.weight
model.layers.23.self_attn.k_proj.weight
model.layers.23.self_attn.v_proj.weight
model.layers.23.self_attn.o_proj.weight
model.layers.23.mlp.gate_proj.weight
model.layers.23.mlp.up_proj.weight
model.layers.23.mlp.down_proj.weight
model.layers.23.input_layernorm.weight
model.layers.23.post_attention_layernorm.weight
model.layers.24.self_attn.q_proj.weight
model.layers.24.self_attn.k_proj.weight
model.layers.24.self_attn.v_proj.weight
model.layers.24.self_attn.o_proj.weight
model.layers.24.mlp.gate_proj.weight
model.layers.24.mlp.up_proj.weight
model.layers.24.mlp.down_proj.weight
model.layers.24.input_layernorm.weight
model.layers.24.post_attention_layernorm.weight
model.layers.25.self_attn.q_proj.weight
model.layers.25.self_attn.k_proj.weight
model.layers.25.self_attn.v_proj.weight
model.layers.25.self_attn.o_proj.weight
model.layers.25.mlp.gate_proj.weight
model.layers.25.mlp.up_proj.weight
model.layers.25.mlp.down_proj.weight
model.layers.25.input_layernorm.weight
model.layers.25.post_attention_layernorm.weight
model.layers.26.self_attn.q_proj.weight
model.layers.26.self_attn.k_proj.weight
model.layers.26.self_attn.v_proj.weight
model.layers.26.self_attn.o_proj.weight
model.layers.26.mlp.gate_proj.weight
model.layers.26.mlp.up_proj.weight
model.layers.26.mlp.down_proj.weight
model.layers.26.input_layernorm.weight
model.layers.26.post_attention_layernorm.weight
model.layers.27.self_attn.q_proj.weight
model.layers.27.self_attn.k_proj.weight
model.layers.27.self_attn.v_proj.weight
model.layers.27.self_attn.o_proj.weight
model.layers.27.mlp.gate_proj.weight
model.layers.27.mlp.up_proj.weight
model.layers.27.mlp.down_proj.weight
model.layers.27.input_layernorm.weight
model.layers.27.post_attention_layernorm.weight
model.layers.28.self_attn.q_proj.weight
model.layers.28.self_attn.k_proj.weight
model.layers.28.self_attn.v_proj.weight
model.layers.28.self_attn.o_proj.weight
model.layers.28.mlp.gate_proj.weight
model.layers.28.mlp.up_proj.weight
model.layers.28.mlp.down_proj.weight
model.layers.28.input_layernorm.weight
model.layers.28.post_attention_layernorm.weight
model.layers.29.self_attn.q_proj.weight
model.layers.29.self_attn.k_proj.weight
model.layers.29.self_attn.v_proj.weight
model.layers.29.self_attn.o_proj.weight
model.layers.29.mlp.gate_proj.weight
model.layers.29.mlp.up_proj.weight
model.layers.29.mlp.down_proj.weight
model.layers.29.input_layernorm.weight
model.layers.29.post_attention_layernorm.weight
model.layers.30.self_attn.q_proj.weight
model.layers.30.self_attn.k_proj.weight
model.layers.30.self_attn.v_proj.weight
model.layers.30.self_attn.o_proj.weight
model.layers.30.mlp.gate_proj.weight
model.layers.30.mlp.up_proj.weight
model.layers.30.mlp.down_proj.weight
model.layers.30.input_layernorm.weight
model.layers.30.post_attention_layernorm.weight
model.layers.31.self_attn.q_proj.weight
model.layers.31.self_attn.k_proj.weight
model.layers.31.self_attn.v_proj.weight
model.layers.31.self_attn.o_proj.weight
model.layers.31.mlp.gate_proj.weight
model.layers.31.mlp.up_proj.weight
model.layers.31.mlp.down_proj.weight
model.layers.31.input_layernorm.weight
model.layers.31.post_attention_layernorm.weight
model.norm.weight
lm_head.weight
wandb: Currently logged in as: johnwicky. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /data5/haoyun.xu/study/MI/MMMI/src/MI/experiment_setup/train_neuron/wandb/run-20231206_080942-3t393k5r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-water-1
wandb: ⭐️ View project at https://wandb.ai/johnwicky/ACL_summary_train_hizh_750step_neuron
wandb: 🚀 View run at https://wandb.ai/johnwicky/ACL_summary_train_hizh_750step_neuron/runs/3t393k5r
  0%|          | 0/3000 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/3000 [00:06<5:07:54,  6.16s/it]  0%|          | 2/3000 [00:10<4:15:37,  5.12s/it]  0%|          | 3/3000 [00:15<4:02:15,  4.85s/it]  0%|          | 4/3000 [00:21<4:35:11,  5.51s/it]  0%|          | 5/3000 [00:25<4:05:30,  4.92s/it]  0%|          | 6/3000 [00:31<4:16:27,  5.14s/it]  0%|          | 7/3000 [00:35<4:08:16,  4.98s/it]  0%|          | 8/3000 [00:42<4:34:22,  5.50s/it]  0%|          | 9/3000 [00:44<3:47:52,  4.57s/it]  0%|          | 10/3000 [00:49<3:46:37,  4.55s/it]                                                   {'loss': 2.2377, 'learning_rate': 1.9933333333333334e-05, 'epoch': 0.01}
  0%|          | 10/3000 [00:49<3:46:37,  4.55s/it]  0%|          | 11/3000 [00:54<3:55:13,  4.72s/it]  0%|          | 12/3000 [00:59<3:54:19,  4.71s/it]  0%|          | 13/3000 [01:04<4:00:31,  4.83s/it]  0%|          | 14/3000 [01:09<4:13:22,  5.09s/it]  0%|          | 15/3000 [01:14<3:58:52,  4.80s/it]  1%|          | 16/3000 [01:20<4:30:20,  5.44s/it]  1%|          | 17/3000 [01:23<3:49:09,  4.61s/it]  1%|          | 18/3000 [01:30<4:20:19,  5.24s/it]  1%|          | 19/3000 [01:33<3:49:49,  4.63s/it]  1%|          | 20/3000 [01:38<3:50:41,  4.64s/it]                                                   {'loss': 2.0319, 'learning_rate': 1.9866666666666667e-05, 'epoch': 0.03}
  1%|          | 20/3000 [01:38<3:50:41,  4.64s/it]  1%|          | 21/3000 [01:43<3:57:46,  4.79s/it]  1%|          | 22/3000 [01:47<3:52:44,  4.69s/it]  1%|          | 23/3000 [01:53<4:13:01,  5.10s/it]  1%|          | 24/3000 [01:59<4:21:28,  5.27s/it]  1%|          | 25/3000 [02:03<4:03:52,  4.92s/it]  1%|          | 26/3000 [02:08<4:01:42,  4.88s/it]  1%|          | 27/3000 [02:13<4:10:22,  5.05s/it]  1%|          | 28/3000 [02:20<4:38:23,  5.62s/it]  1%|          | 29/3000 [02:25<4:20:38,  5.26s/it]  1%|          | 30/3000 [02:28<3:48:44,  4.62s/it]                                                   {'loss': 1.8044, 'learning_rate': 1.98e-05, 'epoch': 0.04}
  1%|          | 30/3000 [02:28<3:48:44,  4.62s/it]  1%|          | 31/3000 [02:34<4:17:30,  5.20s/it]  1%|          | 32/3000 [02:37<3:35:15,  4.35s/it]  1%|          | 33/3000 [02:44<4:14:20,  5.14s/it]  1%|          | 34/3000 [02:47<3:37:56,  4.41s/it]  1%|          | 35/3000 [02:54<4:19:32,  5.25s/it]  1%|          | 36/3000 [02:58<4:00:07,  4.86s/it]  1%|          | 37/3000 [03:04<4:27:04,  5.41s/it]  1%|▏         | 38/3000 [03:08<3:58:52,  4.84s/it]  1%|▏         | 39/3000 [03:15<4:26:32,  5.40s/it]  1%|▏         | 40/3000 [03:18<3:51:07,  4.68s/it]                                                   {'loss': 1.6585, 'learning_rate': 1.9733333333333336e-05, 'epoch': 0.05}
  1%|▏         | 40/3000 [03:18<3:51:07,  4.68s/it]  1%|▏         | 41/3000 [03:23<3:59:44,  4.86s/it]  1%|▏         | 42/3000 [03:27<3:52:58,  4.73s/it]  1%|▏         | 43/3000 [03:33<4:04:56,  4.97s/it]  1%|▏         | 44/3000 [03:39<4:15:28,  5.19s/it]  2%|▏         | 45/3000 [03:44<4:23:31,  5.35s/it]  2%|▏         | 46/3000 [03:49<4:11:22,  5.11s/it]  2%|▏         | 47/3000 [03:56<4:35:48,  5.60s/it]  2%|▏         | 48/3000 [03:59<4:02:46,  4.93s/it]  2%|▏         | 49/3000 [04:04<4:05:51,  5.00s/it]  2%|▏         | 50/3000 [04:08<3:57:12,  4.82s/it]                                                   {'loss': 1.5717, 'learning_rate': 1.9666666666666666e-05, 'epoch': 0.07}
  2%|▏         | 50/3000 [04:09<3:57:12,  4.82s/it]  2%|▏         | 51/3000 [04:15<4:15:02,  5.19s/it]  2%|▏         | 52/3000 [04:18<3:51:42,  4.72s/it]  2%|▏         | 53/3000 [04:25<4:23:45,  5.37s/it]  2%|▏         | 54/3000 [04:28<3:48:29,  4.65s/it]  2%|▏         | 55/3000 [04:35<4:23:31,  5.37s/it]  2%|▏         | 56/3000 [04:38<3:44:15,  4.57s/it]  2%|▏         | 57/3000 [04:45<4:27:24,  5.45s/it]  2%|▏         | 58/3000 [04:47<3:39:47,  4.48s/it]  2%|▏         | 59/3000 [04:54<4:14:14,  5.19s/it]  2%|▏         | 60/3000 [04:57<3:40:44,  4.50s/it]                                                   {'loss': 1.5498, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.08}
  2%|▏         | 60/3000 [04:57<3:40:44,  4.50s/it]  2%|▏         | 61/3000 [05:04<4:12:40,  5.16s/it]  2%|▏         | 62/3000 [05:07<3:41:17,  4.52s/it]  2%|▏         | 63/3000 [05:14<4:18:39,  5.28s/it]  2%|▏         | 64/3000 [05:18<3:52:31,  4.75s/it]  2%|▏         | 65/3000 [05:25<4:27:41,  5.47s/it]  2%|▏         | 66/3000 [05:28<3:49:25,  4.69s/it]  2%|▏         | 67/3000 [05:33<4:01:16,  4.94s/it]  2%|▏         | 68/3000 [05:38<3:55:04,  4.81s/it]  2%|▏         | 69/3000 [05:44<4:12:03,  5.16s/it]  2%|▏         | 70/3000 [05:48<4:06:30,  5.05s/it]                                                   {'loss': 1.6232, 'learning_rate': 1.9533333333333335e-05, 'epoch': 0.09}
  2%|▏         | 70/3000 [05:49<4:06:30,  5.05s/it]  2%|▏         | 71/3000 [05:53<4:04:01,  5.00s/it]  2%|▏         | 72/3000 [05:58<3:57:22,  4.86s/it]  2%|▏         | 73/3000 [06:03<4:09:28,  5.11s/it]  2%|▏         | 74/3000 [06:09<4:09:21,  5.11s/it]  2%|▎         | 75/3000 [06:13<4:06:26,  5.06s/it]
  0%|          | 0/73 [00:00<?, ?it/s][A
  3%|▎         | 2/73 [00:00<00:04, 14.74it/s][A
  5%|▌         | 4/73 [00:00<00:04, 15.05it/s][A
  8%|▊         | 6/73 [00:00<00:06, 10.72it/s][A
 11%|█         | 8/73 [00:00<00:06, 10.07it/s][A
 14%|█▎        | 10/73 [00:00<00:06,  9.01it/s][A
 15%|█▌        | 11/73 [00:01<00:07,  8.69it/s][A
 18%|█▊        | 13/73 [00:01<00:06,  9.14it/s][A
 19%|█▉        | 14/73 [00:01<00:06,  9.24it/s][A
 21%|██        | 15/73 [00:01<00:06,  8.90it/s][A
 22%|██▏       | 16/73 [00:01<00:06,  8.99it/s][A
 23%|██▎       | 17/73 [00:01<00:06,  8.96it/s][A
 25%|██▍       | 18/73 [00:01<00:06,  8.91it/s][A
 26%|██▌       | 19/73 [00:02<00:06,  8.88it/s][A
 27%|██▋       | 20/73 [00:02<00:06,  8.62it/s][A
 29%|██▉       | 21/73 [00:02<00:06,  8.60it/s][A
 32%|███▏      | 23/73 [00:02<00:05,  9.49it/s][A
 33%|███▎      | 24/73 [00:02<00:05,  9.41it/s][A
 36%|███▌      | 26/73 [00:02<00:04, 10.48it/s][A
 38%|███▊      | 28/73 [00:02<00:04, 10.40it/s][A
 41%|████      | 30/73 [00:03<00:04, 10.41it/s][A
 44%|████▍     | 32/73 [00:03<00:04,  9.93it/s][A
 47%|████▋     | 34/73 [00:03<00:03, 10.11it/s][A
 49%|████▉     | 36/73 [00:03<00:03,  9.29it/s][A
 51%|█████     | 37/73 [00:03<00:03,  9.07it/s][A
 52%|█████▏    | 38/73 [00:03<00:03,  9.00it/s][A
 53%|█████▎    | 39/73 [00:04<00:03,  8.91it/s][A
 55%|█████▍    | 40/73 [00:04<00:03,  9.15it/s][A
 56%|█████▌    | 41/73 [00:04<00:03,  9.11it/s][A
 58%|█████▊    | 42/73 [00:04<00:03,  9.32it/s][A
 60%|██████    | 44/73 [00:04<00:03,  9.61it/s][A
 62%|██████▏   | 45/73 [00:04<00:03,  8.69it/s][A
 63%|██████▎   | 46/73 [00:04<00:03,  8.46it/s][A
 64%|██████▍   | 47/73 [00:05<00:03,  8.51it/s][A
 67%|██████▋   | 49/73 [00:05<00:02,  9.26it/s][A
 70%|██████▉   | 51/73 [00:05<00:02,  9.26it/s][A
 71%|███████   | 52/73 [00:05<00:02,  9.13it/s][A
 73%|███████▎  | 53/73 [00:05<00:02,  8.96it/s][A
 74%|███████▍  | 54/73 [00:05<00:02,  8.35it/s][A
 75%|███████▌  | 55/73 [00:05<00:02,  8.53it/s][A
 77%|███████▋  | 56/73 [00:06<00:02,  8.32it/s][A
 78%|███████▊  | 57/73 [00:06<00:01,  8.35it/s][A
 79%|███████▉  | 58/73 [00:06<00:01,  8.18it/s][A
 81%|████████  | 59/73 [00:06<00:01,  8.41it/s][A
 82%|████████▏ | 60/73 [00:06<00:01,  8.60it/s][A
 84%|████████▎ | 61/73 [00:06<00:01,  8.78it/s][A
 85%|████████▍ | 62/73 [00:06<00:01,  8.60it/s][A
 86%|████████▋ | 63/73 [00:06<00:01,  8.49it/s][A
 89%|████████▉ | 65/73 [00:07<00:00,  9.64it/s][A
 92%|█████████▏| 67/73 [00:07<00:00,  9.50it/s][A
 95%|█████████▍| 69/73 [00:07<00:00,  9.60it/s][A
 96%|█████████▌| 70/73 [00:07<00:00,  9.50it/s][A
 97%|█████████▋| 71/73 [00:07<00:00,  9.31it/s][A
100%|██████████| 73/73 [00:07<00:00,  9.90it/s][A                                                   
                                               [A{'eval_loss': 1.5184326171875, 'eval_accuracy': 0.4213838178949741, 'eval_runtime': 11.4807, 'eval_samples_per_second': 37.89, 'eval_steps_per_second': 6.359, 'epoch': 0.1}
  2%|▎         | 75/3000 [06:25<4:06:26,  5.06s/it]
100%|██████████| 73/73 [00:09<00:00,  9.90it/s][A
                                               [A  3%|▎         | 76/3000 [06:28<6:30:16,  8.01s/it]  3%|▎         | 77/3000 [06:34<5:55:30,  7.30s/it]  3%|▎         | 78/3000 [06:39<5:27:28,  6.72s/it]  3%|▎         | 79/3000 [06:44<4:53:42,  6.03s/it]  3%|▎         | 80/3000 [06:50<4:52:24,  6.01s/it]                                                   {'loss': 1.5597, 'learning_rate': 1.9466666666666668e-05, 'epoch': 0.11}
  3%|▎         | 80/3000 [06:50<4:52:24,  6.01s/it]  3%|▎         | 81/3000 [06:55<4:34:59,  5.65s/it]  3%|▎         | 82/3000 [06:59<4:14:24,  5.23s/it]  3%|▎         | 83/3000 [07:04<4:15:26,  5.25s/it]  3%|▎         | 84/3000 [07:09<4:10:01,  5.14s/it]  3%|▎         | 85/3000 [07:15<4:17:52,  5.31s/it]  3%|▎         | 86/3000 [07:20<4:18:03,  5.31s/it]  3%|▎         | 87/3000 [07:25<4:12:36,  5.20s/it]  3%|▎         | 88/3000 [07:29<3:55:20,  4.85s/it]  3%|▎         | 89/3000 [07:35<4:13:18,  5.22s/it]  3%|▎         | 90/3000 [07:39<3:55:25,  4.85s/it]                                                   {'loss': 1.5197, 'learning_rate': 1.94e-05, 'epoch': 0.12}
  3%|▎         | 90/3000 [07:40<3:55:25,  4.85s/it]  3%|▎         | 91/3000 [07:44<3:53:25,  4.81s/it]  3%|▎         | 92/3000 [07:48<3:44:01,  4.62s/it]  3%|▎         | 93/3000 [07:54<4:00:46,  4.97s/it]  3%|▎         | 94/3000 [07:58<3:54:37,  4.84s/it]  3%|▎         | 95/3000 [08:04<4:12:38,  5.22s/it]  3%|▎         | 96/3000 [08:10<4:10:19,  5.17s/it]  3%|▎         | 97/3000 [08:15<4:14:26,  5.26s/it]  3%|▎         | 98/3000 [08:20<4:07:38,  5.12s/it]  3%|▎         | 99/3000 [08:25<4:03:36,  5.04s/it]  3%|▎         | 100/3000 [08:29<3:53:58,  4.84s/it]                                                    {'loss': 1.4815, 'learning_rate': 1.9333333333333333e-05, 'epoch': 0.13}
  3%|▎         | 100/3000 [08:29<3:53:58,  4.84s/it]  3%|▎         | 101/3000 [08:35<4:04:31,  5.06s/it]  3%|▎         | 102/3000 [08:39<3:51:17,  4.79s/it]  3%|▎         | 103/3000 [08:44<4:03:13,  5.04s/it]  3%|▎         | 104/3000 [08:49<3:57:40,  4.92s/it]  4%|▎         | 105/3000 [08:56<4:21:36,  5.42s/it]  4%|▎         | 106/3000 [08:59<3:57:26,  4.92s/it]  4%|▎         | 107/3000 [09:06<4:15:51,  5.31s/it]  4%|▎         | 108/3000 [09:10<4:01:55,  5.02s/it]  4%|▎         | 109/3000 [09:16<4:14:43,  5.29s/it]  4%|▎         | 110/3000 [09:20<4:04:29,  5.08s/it]                                                    {'loss': 1.5398, 'learning_rate': 1.926666666666667e-05, 'epoch': 0.15}
  4%|▎         | 110/3000 [09:21<4:04:29,  5.08s/it]  4%|▎         | 111/3000 [09:26<4:18:53,  5.38s/it]  4%|▎         | 112/3000 [09:31<4:06:18,  5.12s/it]  4%|▍         | 113/3000 [09:37<4:21:21,  5.43s/it]  4%|▍         | 114/3000 [09:42<4:06:40,  5.13s/it]  4%|▍         | 115/3000 [09:49<4:40:55,  5.84s/it]  4%|▍         | 116/3000 [09:52<3:58:34,  4.96s/it]  4%|▍         | 117/3000 [09:59<4:30:36,  5.63s/it]  4%|▍         | 118/3000 [10:02<3:55:05,  4.89s/it]  4%|▍         | 119/3000 [10:08<4:04:40,  5.10s/it]  4%|▍         | 120/3000 [10:12<3:55:52,  4.91s/it]                                                    {'loss': 1.5375, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.16}
  4%|▍         | 120/3000 [10:13<3:55:52,  4.91s/it]  4%|▍         | 121/3000 [10:18<4:04:16,  5.09s/it]  4%|▍         | 122/3000 [10:23<3:58:19,  4.97s/it]  4%|▍         | 123/3000 [10:28<4:11:02,  5.24s/it]  4%|▍         | 124/3000 [10:33<4:02:26,  5.06s/it]  4%|▍         | 125/3000 [10:39<4:18:51,  5.40s/it]  4%|▍         | 126/3000 [10:44<4:02:49,  5.07s/it]  4%|▍         | 127/3000 [10:49<4:13:02,  5.28s/it]  4%|▍         | 128/3000 [10:54<4:00:17,  5.02s/it]  4%|▍         | 129/3000 [11:00<4:22:42,  5.49s/it]  4%|▍         | 130/3000 [11:03<3:47:00,  4.75s/it]                                                    {'loss': 1.5084, 'learning_rate': 1.9133333333333335e-05, 'epoch': 0.17}
  4%|▍         | 130/3000 [11:04<3:47:00,  4.75s/it]  4%|▍         | 131/3000 [11:10<4:14:10,  5.32s/it]  4%|▍         | 132/3000 [11:13<3:37:19,  4.55s/it]  4%|▍         | 133/3000 [11:20<4:10:34,  5.24s/it]  4%|▍         | 134/3000 [11:23<3:37:55,  4.56s/it]  4%|▍         | 135/3000 [11:30<4:16:32,  5.37s/it]  5%|▍         | 136/3000 [11:33<3:41:44,  4.65s/it]  5%|▍         | 137/3000 [11:39<4:10:05,  5.24s/it]  5%|▍         | 138/3000 [11:42<3:37:32,  4.56s/it]  5%|▍         | 139/3000 [11:50<4:16:40,  5.38s/it]  5%|▍         | 140/3000 [11:53<3:47:37,  4.78s/it]                                                    {'loss': 1.4224, 'learning_rate': 1.9066666666666668e-05, 'epoch': 0.19}
  5%|▍         | 140/3000 [11:53<3:47:37,  4.78s/it]  5%|▍         | 141/3000 [11:59<4:06:59,  5.18s/it]  5%|▍         | 142/3000 [12:03<3:49:02,  4.81s/it]  5%|▍         | 143/3000 [12:09<4:04:41,  5.14s/it]  5%|▍         | 144/3000 [12:14<4:04:15,  5.13s/it]  5%|▍         | 145/3000 [12:18<3:51:16,  4.86s/it]  5%|▍         | 146/3000 [12:26<4:25:28,  5.58s/it]  5%|▍         | 147/3000 [12:29<3:59:06,  5.03s/it]  5%|▍         | 148/3000 [12:36<4:20:32,  5.48s/it]  5%|▍         | 149/3000 [12:39<3:51:30,  4.87s/it]  5%|▌         | 150/3000 [12:45<3:58:47,  5.03s/it]                                                    {'loss': 1.4155, 'learning_rate': 1.9e-05, 'epoch': 0.2}
  5%|▌         | 150/3000 [12:45<3:58:47,  5.03s/it]
  0%|          | 0/73 [00:00<?, ?it/s][A
  3%|▎         | 2/73 [00:00<00:04, 16.47it/s][A
  5%|▌         | 4/73 [00:00<00:04, 15.74it/s][A
  8%|▊         | 6/73 [00:00<00:06, 10.70it/s][A
 11%|█         | 8/73 [00:00<00:06, 10.07it/s][A
 14%|█▎        | 10/73 [00:00<00:07,  8.99it/s][A
 15%|█▌        | 11/73 [00:01<00:07,  8.67it/s][A
 18%|█▊        | 13/73 [00:01<00:06,  9.12it/s][A
 19%|█▉        | 14/73 [00:01<00:06,  9.23it/s][A
 21%|██        | 15/73 [00:01<00:06,  8.89it/s][A
 22%|██▏       | 16/73 [00:01<00:06,  9.00it/s][A
 23%|██▎       | 17/73 [00:01<00:06,  8.96it/s][A
 25%|██▍       | 18/73 [00:01<00:06,  8.92it/s][A
 26%|██▌       | 19/73 [00:02<00:06,  8.88it/s][A
 27%|██▋       | 20/73 [00:02<00:06,  8.60it/s][A
 29%|██▉       | 21/73 [00:02<00:06,  8.59it/s][A
 32%|███▏      | 23/73 [00:02<00:05,  9.47it/s][A
 33%|███▎      | 24/73 [00:02<00:05,  9.38it/s][A
 36%|███▌      | 26/73 [00:02<00:04, 10.48it/s][A
 38%|███▊      | 28/73 [00:02<00:04, 10.39it/s][A
 41%|████      | 30/73 [00:03<00:04, 10.38it/s][A
 44%|████▍     | 32/73 [00:03<00:04,  9.91it/s][A
 47%|████▋     | 34/73 [00:03<00:03, 10.09it/s][A
 49%|████▉     | 36/73 [00:03<00:03,  9.28it/s][A
 51%|█████     | 37/73 [00:03<00:03,  9.06it/s][A
 52%|█████▏    | 38/73 [00:03<00:03,  8.99it/s][A
 53%|█████▎    | 39/73 [00:04<00:03,  8.91it/s][A
 55%|█████▍    | 40/73 [00:04<00:03,  9.15it/s][A
 56%|█████▌    | 41/73 [00:04<00:03,  9.12it/s][A
 58%|█████▊    | 42/73 [00:04<00:03,  9.32it/s][A
 60%|██████    | 44/73 [00:04<00:03,  9.60it/s][A
 62%|██████▏   | 45/73 [00:04<00:03,  8.70it/s][A
 63%|██████▎   | 46/73 [00:04<00:03,  8.46it/s][A
 64%|██████▍   | 47/73 [00:05<00:03,  8.52it/s][A
 67%|██████▋   | 49/73 [00:05<00:02,  9.25it/s][A
 70%|██████▉   | 51/73 [00:05<00:02,  9.24it/s][A
 71%|███████   | 52/73 [00:05<00:02,  9.11it/s][A
 73%|███████▎  | 53/73 [00:05<00:02,  8.95it/s][A
 74%|███████▍  | 54/73 [00:05<00:02,  8.34it/s][A
 75%|███████▌  | 55/73 [00:05<00:02,  8.53it/s][A
 77%|███████▋  | 56/73 [00:06<00:02,  8.30it/s][A
 78%|███████▊  | 57/73 [00:06<00:01,  8.33it/s][A
 79%|███████▉  | 58/73 [00:06<00:01,  8.16it/s][A
 81%|████████  | 59/73 [00:06<00:01,  8.40it/s][A
 82%|████████▏ | 60/73 [00:06<00:01,  8.60it/s][A
 84%|████████▎ | 61/73 [00:06<00:01,  8.77it/s][A
 85%|████████▍ | 62/73 [00:06<00:01,  8.60it/s][A
 86%|████████▋ | 63/73 [00:06<00:01,  8.48it/s][A
 89%|████████▉ | 65/73 [00:07<00:00,  9.63it/s][A
 92%|█████████▏| 67/73 [00:07<00:00,  9.49it/s][A
 95%|█████████▍| 69/73 [00:07<00:00,  9.58it/s][A
 96%|█████████▌| 70/73 [00:07<00:00,  9.48it/s][A
 97%|█████████▋| 71/73 [00:07<00:00,  9.30it/s][A
100%|██████████| 73/73 [00:07<00:00,  9.89it/s][A                                                    
                                               [A{'eval_loss': 1.4429042339324951, 'eval_accuracy': 0.4230486064157464, 'eval_runtime': 11.3764, 'eval_samples_per_second': 38.237, 'eval_steps_per_second': 6.417, 'epoch': 0.2}
  5%|▌         | 150/3000 [12:56<3:58:47,  5.03s/it]
100%|██████████| 73/73 [00:09<00:00,  9.89it/s][A
                                               [A  5%|▌         | 151/3000 [12:59<6:10:24,  7.80s/it]  5%|▌         | 152/3000 [13:05<5:43:44,  7.24s/it]  5%|▌         | 153/3000 [13:09<4:57:57,  6.28s/it]  5%|▌         | 154/3000 [13:15<4:53:47,  6.19s/it]  5%|▌         | 155/3000 [13:21<4:46:28,  6.04s/it]  5%|▌         | 156/3000 [13:25<4:21:51,  5.52s/it]  5%|▌         | 157/3000 [13:31<4:31:09,  5.72s/it]  5%|▌         | 158/3000 [13:37<4:38:38,  5.88s/it]  5%|▌         | 159/3000 [13:42<4:13:40,  5.36s/it]  5%|▌         | 160/3000 [13:45<3:51:24,  4.89s/it]                                                    {'loss': 1.4312, 'learning_rate': 1.8933333333333334e-05, 'epoch': 0.21}
  5%|▌         | 160/3000 [13:46<3:51:24,  4.89s/it]  5%|▌         | 161/3000 [13:51<3:54:27,  4.96s/it]  5%|▌         | 162/3000 [13:55<3:50:43,  4.88s/it]  5%|▌         | 163/3000 [14:02<4:17:03,  5.44s/it]  5%|▌         | 164/3000 [14:05<3:38:30,  4.62s/it]  6%|▌         | 165/3000 [14:11<4:05:18,  5.19s/it]  6%|▌         | 166/3000 [14:15<3:46:55,  4.80s/it]  6%|▌         | 167/3000 [14:22<4:11:57,  5.34s/it]  6%|▌         | 168/3000 [14:25<3:36:55,  4.60s/it]  6%|▌         | 169/3000 [14:32<4:11:25,  5.33s/it]  6%|▌         | 170/3000 [14:34<3:35:39,  4.57s/it]                                                    {'loss': 1.3779, 'learning_rate': 1.886666666666667e-05, 'epoch': 0.23}
  6%|▌         | 170/3000 [14:34<3:35:39,  4.57s/it]  6%|▌         | 171/3000 [14:41<4:08:18,  5.27s/it]  6%|▌         | 172/3000 [14:45<3:51:40,  4.92s/it]  6%|▌         | 173/3000 [14:50<3:42:48,  4.73s/it]  6%|▌         | 174/3000 [14:56<4:04:34,  5.19s/it]  6%|▌         | 175/3000 [14:59<3:31:46,  4.50s/it]  6%|▌         | 176/3000 [15:05<3:54:35,  4.98s/it]  6%|▌         | 177/3000 [15:08<3:22:13,  4.30s/it]  6%|▌         | 178/3000 [15:15<4:00:13,  5.11s/it]  6%|▌         | 179/3000 [15:17<3:27:49,  4.42s/it]  6%|▌         | 180/3000 [15:24<4:03:40,  5.18s/it]                                                    {'loss': 1.3301, 'learning_rate': 1.88e-05, 'epoch': 0.24}
  6%|▌         | 180/3000 [15:25<4:03:40,  5.18s/it]  6%|▌         | 181/3000 [15:28<3:44:30,  4.78s/it]  6%|▌         | 182/3000 [15:35<4:07:02,  5.26s/it]  6%|▌         | 183/3000 [15:39<3:59:37,  5.10s/it]  6%|▌         | 184/3000 [15:45<4:09:59,  5.33s/it]  6%|▌         | 185/3000 [15:50<3:57:15,  5.06s/it]  6%|▌         | 186/3000 [15:55<4:06:27,  5.25s/it]  6%|▌         | 187/3000 [16:00<3:54:53,  5.01s/it]  6%|▋         | 188/3000 [16:04<3:46:43,  4.84s/it]  6%|▋         | 189/3000 [16:11<4:07:14,  5.28s/it]  6%|▋         | 190/3000 [16:15<3:57:31,  5.07s/it]                                                    {'loss': 1.4268, 'learning_rate': 1.8733333333333336e-05, 'epoch': 0.25}
  6%|▋         | 190/3000 [16:15<3:57:31,  5.07s/it]  6%|▋         | 191/3000 [16:21<4:08:18,  5.30s/it]  6%|▋         | 192/3000 [16:27<4:11:58,  5.38s/it]  6%|▋         | 193/3000 [16:31<4:03:50,  5.21s/it]  6%|▋         | 194/3000 [16:37<4:05:13,  5.24s/it]  6%|▋         | 195/3000 [16:42<3:59:53,  5.13s/it]  7%|▋         | 196/3000 [16:48<4:15:59,  5.48s/it]  7%|▋         | 197/3000 [16:52<3:56:33,  5.06s/it]  7%|▋         | 198/3000 [16:56<3:48:08,  4.89s/it]  7%|▋         | 199/3000 [17:02<3:54:49,  5.03s/it]  7%|▋         | 200/3000 [17:06<3:47:40,  4.88s/it]                                                    {'loss': 1.3794, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.27}
  7%|▋         | 200/3000 [17:06<3:47:40,  4.88s/it]  7%|▋         | 201/3000 [17:12<4:02:56,  5.21s/it]  7%|▋         | 202/3000 [17:17<4:02:35,  5.20s/it]  7%|▋         | 203/3000 [17:22<3:50:08,  4.94s/it]  7%|▋         | 204/3000 [17:28<4:04:16,  5.24s/it]  7%|▋         | 205/3000 [17:32<3:56:21,  5.07s/it]  7%|▋         | 206/3000 [17:39<4:11:36,  5.40s/it]  7%|▋         | 207/3000 [17:41<3:34:34,  4.61s/it]  7%|▋         | 208/3000 [17:49<4:13:35,  5.45s/it]  7%|▋         | 209/3000 [17:52<3:37:54,  4.68s/it]  7%|▋         | 210/3000 [17:59<4:10:22,  5.38s/it]                                                    {'loss': 1.3726, 'learning_rate': 1.86e-05, 'epoch': 0.28}
  7%|▋         | 210/3000 [17:59<4:10:22,  5.38s/it]  7%|▋         | 211/3000 [18:02<3:48:44,  4.92s/it]  7%|▋         | 212/3000 [18:09<4:04:49,  5.27s/it]  7%|▋         | 213/3000 [18:14<4:03:38,  5.25s/it]  7%|▋         | 214/3000 [18:19<4:09:31,  5.37s/it]  7%|▋         | 215/3000 [18:24<3:59:18,  5.16s/it]  7%|▋         | 216/3000 [18:29<3:54:36,  5.06s/it]  7%|▋         | 217/3000 [18:33<3:43:26,  4.82s/it]  7%|▋         | 218/3000 [18:39<3:52:15,  5.01s/it]  7%|▋         | 219/3000 [18:43<3:45:57,  4.87s/it]  7%|▋         | 220/3000 [18:49<4:03:05,  5.25s/it]                                                    {'loss': 1.4443, 'learning_rate': 1.8533333333333334e-05, 'epoch': 0.29}
  7%|▋         | 220/3000 [18:49<4:03:05,  5.25s/it]  7%|▋         | 221/3000 [18:54<3:49:28,  4.95s/it]  7%|▋         | 222/3000 [19:00<4:03:06,  5.25s/it]  7%|▋         | 223/3000 [19:05<4:12:31,  5.46s/it]  7%|▋         | 224/3000 [19:11<4:16:18,  5.54s/it]  8%|▊         | 225/3000 [19:16<4:02:06,  5.23s/it]
  0%|          | 0/73 [00:00<?, ?it/s][A
  3%|▎         | 2/73 [00:00<00:04, 16.69it/s][A
  5%|▌         | 4/73 [00:00<00:04, 15.83it/s][A
  8%|▊         | 6/73 [00:00<00:06, 10.92it/s][A
 11%|█         | 8/73 [00:00<00:06, 10.18it/s][A
 14%|█▎        | 10/73 [00:00<00:06,  9.06it/s][A
 15%|█▌        | 11/73 [00:01<00:07,  8.73it/s][A
 18%|█▊        | 13/73 [00:01<00:06,  9.17it/s][A
 19%|█▉        | 14/73 [00:01<00:06,  9.27it/s][A
 21%|██        | 15/73 [00:01<00:06,  8.93it/s][A
 22%|██▏       | 16/73 [00:01<00:06,  9.02it/s][A
 23%|██▎       | 17/73 [00:01<00:06,  8.98it/s][A
 25%|██▍       | 18/73 [00:01<00:06,  8.93it/s][A
 26%|██▌       | 19/73 [00:01<00:06,  8.89it/s][A
 27%|██▋       | 20/73 [00:02<00:06,  8.63it/s][A
 29%|██▉       | 21/73 [00:02<00:06,  8.61it/s][A
 32%|███▏      | 23/73 [00:02<00:05,  9.49it/s][A
 33%|███▎      | 24/73 [00:02<00:05,  9.40it/s][A
 36%|███▌      | 26/73 [00:02<00:04, 10.48it/s][A
 38%|███▊      | 28/73 [00:02<00:04, 10.41it/s][A
 41%|████      | 30/73 [00:03<00:04, 10.42it/s][A
 44%|████▍     | 32/73 [00:03<00:04,  9.94it/s][A
 47%|████▋     | 34/73 [00:03<00:03, 10.12it/s][A
 49%|████▉     | 36/73 [00:03<00:03,  9.29it/s][A
 51%|█████     | 37/73 [00:03<00:03,  9.08it/s][A
 52%|█████▏    | 38/73 [00:03<00:03,  9.00it/s][A
 53%|█████▎    | 39/73 [00:04<00:03,  8.91it/s][A
 55%|█████▍    | 40/73 [00:04<00:03,  9.15it/s][A
 56%|█████▌    | 41/73 [00:04<00:03,  9.11it/s][A
 58%|█████▊    | 42/73 [00:04<00:03,  9.31it/s][A
 60%|██████    | 44/73 [00:04<00:03,  9.60it/s][A
 62%|██████▏   | 45/73 [00:04<00:03,  8.69it/s][A
 63%|██████▎   | 46/73 [00:04<00:03,  8.46it/s][A
 64%|██████▍   | 47/73 [00:04<00:03,  8.51it/s][A
 67%|██████▋   | 49/73 [00:05<00:02,  9.26it/s][A
 70%|██████▉   | 51/73 [00:05<00:02,  9.26it/s][A
 71%|███████   | 52/73 [00:05<00:02,  9.12it/s][A
 73%|███████▎  | 53/73 [00:05<00:02,  8.96it/s][A
 74%|███████▍  | 54/73 [00:05<00:02,  8.35it/s][A
 75%|███████▌  | 55/73 [00:05<00:02,  8.53it/s][A
 77%|███████▋  | 56/73 [00:06<00:02,  8.31it/s][A
 78%|███████▊  | 57/73 [00:06<00:01,  8.34it/s][A
 79%|███████▉  | 58/73 [00:06<00:01,  8.17it/s][A
 81%|████████  | 59/73 [00:06<00:01,  8.39it/s][A
 82%|████████▏ | 60/73 [00:06<00:01,  8.60it/s][A
 84%|████████▎ | 61/73 [00:06<00:01,  8.78it/s][A
 85%|████████▍ | 62/73 [00:06<00:01,  8.59it/s][A
 86%|████████▋ | 63/73 [00:06<00:01,  8.49it/s][A
 89%|████████▉ | 65/73 [00:07<00:00,  9.64it/s][A
 92%|█████████▏| 67/73 [00:07<00:00,  9.51it/s][A
 95%|█████████▍| 69/73 [00:07<00:00,  9.60it/s][A
 96%|█████████▌| 70/73 [00:07<00:00,  9.50it/s][A
 97%|█████████▋| 71/73 [00:07<00:00,  9.31it/s][A
100%|██████████| 73/73 [00:07<00:00,  9.90it/s][A                                                    
                                               [A{'eval_loss': 1.3915114402770996, 'eval_accuracy': 0.42394711141161445, 'eval_runtime': 11.5565, 'eval_samples_per_second': 37.641, 'eval_steps_per_second': 6.317, 'epoch': 0.3}
  8%|▊         | 225/3000 [19:27<4:02:06,  5.23s/it]
100%|██████████| 73/73 [00:09<00:00,  9.90it/s][A
                                               [A  8%|▊         | 226/3000 [19:31<6:17:18,  8.16s/it]  8%|▊         | 227/3000 [19:35<5:25:51,  7.05s/it]  8%|▊         | 228/3000 [19:40<4:50:00,  6.28s/it]  8%|▊         | 229/3000 [19:46<4:45:57,  6.19s/it]  8%|▊         | 230/3000 [19:50<4:19:55,  5.63s/it]                                                    {'loss': 1.4287, 'learning_rate': 1.8466666666666667e-05, 'epoch': 0.31}
  8%|▊         | 230/3000 [19:50<4:19:55,  5.63s/it]  8%|▊         | 231/3000 [19:56<4:23:25,  5.71s/it]  8%|▊         | 232/3000 [20:03<4:40:31,  6.08s/it]  8%|▊         | 233/3000 [20:06<4:01:34,  5.24s/it]  8%|▊         | 234/3000 [20:11<4:01:31,  5.24s/it]  8%|▊         | 235/3000 [20:16<3:49:51,  4.99s/it]  8%|▊         | 236/3000 [20:22<4:14:14,  5.52s/it]  8%|▊         | 237/3000 [20:26<3:45:50,  4.90s/it]  8%|▊         | 238/3000 [20:33<4:21:17,  5.68s/it]  8%|▊         | 239/3000 [20:36<3:45:29,  4.90s/it]  8%|▊         | 240/3000 [20:43<4:01:43,  5.25s/it]                                                    {'loss': 1.4051, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.32}
  8%|▊         | 240/3000 [20:43<4:01:43,  5.25s/it]  8%|▊         | 241/3000 [20:47<3:48:34,  4.97s/it]  8%|▊         | 242/3000 [20:54<4:17:19,  5.60s/it]  8%|▊         | 243/3000 [20:57<3:39:15,  4.77s/it]  8%|▊         | 244/3000 [21:04<4:08:46,  5.42s/it]  8%|▊         | 245/3000 [21:07<3:33:53,  4.66s/it]  8%|▊         | 246/3000 [21:13<3:52:03,  5.06s/it]  8%|▊         | 247/3000 [21:15<3:20:38,  4.37s/it]  8%|▊         | 248/3000 [21:22<3:56:11,  5.15s/it]  8%|▊         | 249/3000 [21:26<3:34:38,  4.68s/it]  8%|▊         | 250/3000 [21:31<3:46:15,  4.94s/it]                                                    {'loss': 1.3809, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.33}
  8%|▊         | 250/3000 [21:32<3:46:15,  4.94s/it]  8%|▊         | 251/3000 [21:36<3:34:53,  4.69s/it]  8%|▊         | 252/3000 [21:42<3:52:26,  5.08s/it]  8%|▊         | 253/3000 [21:46<3:41:14,  4.83s/it]  8%|▊         | 254/3000 [21:53<4:07:27,  5.41s/it]  8%|▊         | 255/3000 [21:57<3:48:33,  5.00s/it]  9%|▊         | 256/3000 [22:02<3:58:31,  5.22s/it]  9%|▊         | 257/3000 [22:07<3:45:10,  4.93s/it]  9%|▊         | 258/3000 [22:12<3:57:09,  5.19s/it]  9%|▊         | 259/3000 [22:17<3:47:47,  4.99s/it]  9%|▊         | 260/3000 [22:23<4:02:09,  5.30s/it]                                                    {'loss': 1.4274, 'learning_rate': 1.826666666666667e-05, 'epoch': 0.35}
  9%|▊         | 260/3000 [22:23<4:02:09,  5.30s/it]  9%|▊         | 261/3000 [22:27<3:44:45,  4.92s/it]  9%|▊         | 262/3000 [22:34<4:08:15,  5.44s/it]  9%|▉         | 263/3000 [22:36<3:30:52,  4.62s/it]  9%|▉         | 264/3000 [22:43<4:04:29,  5.36s/it]  9%|▉         | 265/3000 [22:47<3:46:51,  4.98s/it]  9%|▉         | 266/3000 [22:54<4:04:29,  5.37s/it]  9%|▉         | 267/3000 [22:57<3:29:41,  4.60s/it]  9%|▉         | 268/3000 [23:04<4:02:05,  5.32s/it]  9%|▉         | 269/3000 [23:05<3:14:16,  4.27s/it]  9%|▉         | 270/3000 [23:12<3:45:49,  4.96s/it]                                                    {'loss': 1.4112, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.36}
  9%|▉         | 270/3000 [23:12<3:45:49,  4.96s/it]  9%|▉         | 271/3000 [23:15<3:12:58,  4.24s/it]  9%|▉         | 272/3000 [23:22<3:51:50,  5.10s/it]  9%|▉         | 273/3000 [23:25<3:22:54,  4.46s/it]  9%|▉         | 274/3000 [23:32<3:57:17,  5.22s/it]  9%|▉         | 275/3000 [23:34<3:20:16,  4.41s/it]  9%|▉         | 276/3000 [23:41<3:48:04,  5.02s/it]  9%|▉         | 277/3000 [23:43<3:18:13,  4.37s/it]  9%|▉         | 278/3000 [23:50<3:55:11,  5.18s/it]  9%|▉         | 279/3000 [23:54<3:33:11,  4.70s/it]  9%|▉         | 280/3000 [24:00<3:53:30,  5.15s/it]                                                    {'loss': 1.3753, 'learning_rate': 1.8133333333333335e-05, 'epoch': 0.37}
  9%|▉         | 280/3000 [24:00<3:53:30,  5.15s/it]  9%|▉         | 281/3000 [24:05<3:53:20,  5.15s/it]  9%|▉         | 282/3000 [24:10<3:40:13,  4.86s/it]  9%|▉         | 283/3000 [24:16<3:55:42,  5.21s/it]  9%|▉         | 284/3000 [24:20<3:45:19,  4.98s/it] 10%|▉         | 285/3000 [24:26<3:53:26,  5.16s/it] 10%|▉         | 286/3000 [24:30<3:39:53,  4.86s/it] 10%|▉         | 287/3000 [24:37<4:16:11,  5.67s/it] 10%|▉         | 288/3000 [24:41<3:48:58,  5.07s/it] 10%|▉         | 289/3000 [24:46<3:52:45,  5.15s/it] 10%|▉         | 290/3000 [24:51<3:42:55,  4.94s/it]                                                    {'loss': 1.3394, 'learning_rate': 1.8066666666666668e-05, 'epoch': 0.39}
 10%|▉         | 290/3000 [24:51<3:42:55,  4.94s/it] 10%|▉         | 291/3000 [24:56<3:51:08,  5.12s/it] 10%|▉         | 292/3000 [25:01<3:41:44,  4.91s/it] 10%|▉         | 293/3000 [25:07<3:57:03,  5.25s/it] 10%|▉         | 294/3000 [25:12<3:59:46,  5.32s/it] 10%|▉         | 295/3000 [25:17<3:53:01,  5.17s/it] 10%|▉         | 296/3000 [25:21<3:42:11,  4.93s/it] 10%|▉         | 297/3000 [25:29<4:12:35,  5.61s/it] 10%|▉         | 298/3000 [25:32<3:42:58,  4.95s/it] 10%|▉         | 299/3000 [25:39<4:09:36,  5.54s/it] 10%|█         | 300/3000 [25:43<3:44:50,  5.00s/it]                                                    {'loss': 1.42, 'learning_rate': 1.8e-05, 'epoch': 0.4}
 10%|█         | 300/3000 [25:43<3:44:50,  5.00s/it]
  0%|          | 0/73 [00:00<?, ?it/s][A
  3%|▎         | 2/73 [00:00<00:04, 16.58it/s][A
  5%|▌         | 4/73 [00:00<00:04, 15.79it/s][A
  8%|▊         | 6/73 [00:00<00:06, 10.90it/s][A
 11%|█         | 8/73 [00:00<00:06, 10.17it/s][A
 14%|█▎        | 10/73 [00:00<00:06,  9.05it/s][A
 15%|█▌        | 11/73 [00:01<00:07,  8.72it/s][A
 18%|█▊        | 13/73 [00:01<00:06,  9.15it/s][A
 19%|█▉        | 14/73 [00:01<00:06,  9.25it/s][A
 21%|██        | 15/73 [00:01<00:06,  8.91it/s][A
 22%|██▏       | 16/73 [00:01<00:06,  9.01it/s][A
 23%|██▎       | 17/73 [00:01<00:06,  8.96it/s][A
 25%|██▍       | 18/73 [00:01<00:06,  8.92it/s][A
 26%|██▌       | 19/73 [00:01<00:06,  8.87it/s][A
 27%|██▋       | 20/73 [00:02<00:06,  8.61it/s][A
 29%|██▉       | 21/73 [00:02<00:06,  8.60it/s][A
 32%|███▏      | 23/73 [00:02<00:05,  9.48it/s][A
 33%|███▎      | 24/73 [00:02<00:05,  9.41it/s][A
 36%|███▌      | 26/73 [00:02<00:04, 10.49it/s][A
 38%|███▊      | 28/73 [00:02<00:04, 10.40it/s][A
 41%|████      | 30/73 [00:03<00:04, 10.41it/s][A
 44%|████▍     | 32/73 [00:03<00:04,  9.93it/s][A
 47%|████▋     | 34/73 [00:03<00:03, 10.11it/s][A
 49%|████▉     | 36/73 [00:03<00:03,  9.28it/s][A
 51%|█████     | 37/73 [00:03<00:03,  9.07it/s][A
 52%|█████▏    | 38/73 [00:03<00:03,  8.99it/s][A
 53%|█████▎    | 39/73 [00:04<00:03,  8.90it/s][A
 55%|█████▍    | 40/73 [00:04<00:03,  9.14it/s][A
 56%|█████▌    | 41/73 [00:04<00:03,  9.11it/s][A
 58%|█████▊    | 42/73 [00:04<00:03,  9.32it/s][A
 60%|██████    | 44/73 [00:04<00:03,  9.60it/s][A
 62%|██████▏   | 45/73 [00:04<00:03,  8.65it/s][A
 63%|██████▎   | 46/73 [00:04<00:03,  8.42it/s][A
 64%|██████▍   | 47/73 [00:04<00:03,  8.48it/s][A
 67%|██████▋   | 49/73 [00:05<00:02,  9.23it/s][A
 70%|██████▉   | 51/73 [00:05<00:02,  9.25it/s][A
 71%|███████   | 52/73 [00:05<00:02,  9.11it/s][A
 73%|███████▎  | 53/73 [00:05<00:02,  8.95it/s][A
 74%|███████▍  | 54/73 [00:05<00:02,  8.34it/s][A
 75%|███████▌  | 55/73 [00:05<00:02,  8.53it/s][A
 77%|███████▋  | 56/73 [00:06<00:02,  8.30it/s][A
 78%|███████▊  | 57/73 [00:06<00:01,  8.34it/s][A
 79%|███████▉  | 58/73 [00:06<00:01,  8.16it/s][A
 81%|████████  | 59/73 [00:06<00:01,  8.40it/s][A
 82%|████████▏ | 60/73 [00:06<00:01,  8.60it/s][A
 84%|████████▎ | 61/73 [00:06<00:01,  8.78it/s][A
 85%|████████▍ | 62/73 [00:06<00:01,  8.59it/s][A
 86%|████████▋ | 63/73 [00:06<00:01,  8.48it/s][A
 89%|████████▉ | 65/73 [00:07<00:00,  9.63it/s][A
 90%|█████████ | 66/73 [00:07<00:00,  9.72it/s][A
 92%|█████████▏| 67/73 [00:07<00:00,  9.42it/s][A
 95%|█████████▍| 69/73 [00:07<00:00,  9.56it/s][A
 96%|█████████▌| 70/73 [00:07<00:00,  9.45it/s][A
 97%|█████████▋| 71/73 [00:07<00:00,  9.26it/s][A
100%|██████████| 73/73 [00:07<00:00,  9.86it/s][A                                                    
                                               [A{'eval_loss': 1.3664666414260864, 'eval_accuracy': 0.42420554428668017, 'eval_runtime': 11.4808, 'eval_samples_per_second': 37.889, 'eval_steps_per_second': 6.358, 'epoch': 0.4}
 10%|█         | 300/3000 [25:54<3:44:50,  5.00s/it]
100%|██████████| 73/73 [00:09<00:00,  9.86it/s][A
                                               [A 10%|█         | 301/3000 [25:58<6:02:31,  8.06s/it] 10%|█         | 302/3000 [26:02<5:11:19,  6.92s/it] 10%|█         | 303/3000 [26:09<5:09:01,  6.87s/it] 10%|█         | 304/3000 [26:12<4:17:34,  5.73s/it] 10%|█         | 305/3000 [26:19<4:27:10,  5.95s/it] 10%|█         | 306/3000 [26:21<3:41:23,  4.93s/it] 10%|█         | 307/3000 [26:28<4:06:58,  5.50s/it] 10%|█         | 308/3000 [26:31<3:31:25,  4.71s/it] 10%|█         | 309/3000 [26:38<3:59:26,  5.34s/it] 10%|█         | 310/3000 [26:40<3:22:08,  4.51s/it]                                                    {'loss': 1.3078, 'learning_rate': 1.7933333333333333e-05, 'epoch': 0.41}
 10%|█         | 310/3000 [26:40<3:22:08,  4.51s/it] 10%|█         | 311/3000 [26:47<3:48:20,  5.10s/it] 10%|█         | 312/3000 [26:49<3:12:59,  4.31s/it] 10%|█         | 313/3000 [26:56<3:54:01,  5.23s/it] 10%|█         | 314/3000 [27:00<3:34:17,  4.79s/it] 10%|█         | 315/3000 [27:06<3:53:51,  5.23s/it] 11%|█         | 316/3000 [27:10<3:27:09,  4.63s/it] 11%|█         | 317/3000 [27:16<3:51:26,  5.18s/it] 11%|█         | 318/3000 [27:19<3:17:04,  4.41s/it] 11%|█         | 319/3000 [27:26<3:52:15,  5.20s/it] 11%|█         | 320/3000 [27:29<3:20:29,  4.49s/it]                                                    {'loss': 1.3242, 'learning_rate': 1.7866666666666666e-05, 'epoch': 0.43}
 11%|█         | 320/3000 [27:29<3:20:29,  4.49s/it] 11%|█         | 321/3000 [27:35<3:47:39,  5.10s/it] 11%|█         | 322/3000 [27:38<3:19:17,  4.47s/it] 11%|█         | 323/3000 [27:45<3:47:11,  5.09s/it] 11%|█         | 324/3000 [27:48<3:19:05,  4.46s/it] 11%|█         | 325/3000 [27:54<3:48:39,  5.13s/it] 11%|█         | 326/3000 [27:58<3:30:26,  4.72s/it] 11%|█         | 327/3000 [28:04<3:47:43,  5.11s/it] 11%|█         | 328/3000 [28:09<3:39:40,  4.93s/it] 11%|█         | 329/3000 [28:15<3:56:46,  5.32s/it] 11%|█         | 330/3000 [28:20<3:53:43,  5.25s/it]                                                    {'loss': 1.3389, 'learning_rate': 1.7800000000000002e-05, 'epoch': 0.44}
 11%|█         | 330/3000 [28:20<3:53:43,  5.25s/it] 11%|█         | 331/3000 [28:24<3:35:48,  4.85s/it] 11%|█         | 332/3000 [28:30<3:52:08,  5.22s/it] 11%|█         | 333/3000 [28:34<3:42:08,  5.00s/it] 11%|█         | 334/3000 [28:41<4:00:29,  5.41s/it] 11%|█         | 335/3000 [28:43<3:19:02,  4.48s/it] 11%|█         | 336/3000 [28:50<3:52:25,  5.23s/it] 11%|█         | 337/3000 [28:54<3:30:23,  4.74s/it] 11%|█▏        | 338/3000 [29:00<3:45:47,  5.09s/it] 11%|█▏        | 339/3000 [29:05<3:50:07,  5.19s/it] 11%|█▏        | 340/3000 [29:09<3:39:38,  4.95s/it]                                                    {'loss': 1.3346, 'learning_rate': 1.7733333333333335e-05, 'epoch': 0.45}
 11%|█▏        | 340/3000 [29:10<3:39:38,  4.95s/it] 11%|█▏        | 341/3000 [29:14<3:38:54,  4.94s/it] 11%|█▏        | 342/3000 [29:19<3:40:37,  4.98s/it] 11%|█▏        | 343/3000 [29:25<3:41:46,  5.01s/it] 11%|█▏        | 344/3000 [29:29<3:35:02,  4.86s/it] 12%|█▏        | 345/3000 [29:35<3:55:36,  5.32s/it] 12%|█▏        | 346/3000 [29:38<3:23:14,  4.59s/it] 12%|█▏        | 347/3000 [29:45<3:51:22,  5.23s/it] 12%|█▏        | 348/3000 [29:49<3:28:54,  4.73s/it] 12%|█▏        | 349/3000 [29:56<4:00:54,  5.45s/it] 12%|█▏        | 350/3000 [29:59<3:30:37,  4.77s/it]                                                    {'loss': 1.2741, 'learning_rate': 1.7666666666666668e-05, 'epoch': 0.47}
 12%|█▏        | 350/3000 [29:59<3:30:37,  4.77s/it] 12%|█▏        | 351/3000 [30:04<3:30:48,  4.77s/it] 12%|█▏        | 352/3000 [30:08<3:27:51,  4.71s/it] 12%|█▏        | 353/3000 [30:14<3:43:41,  5.07s/it] 12%|█▏        | 354/3000 [30:19<3:36:17,  4.90s/it] 12%|█▏        | 355/3000 [30:25<3:48:27,  5.18s/it] 12%|█▏        | 356/3000 [30:29<3:41:22,  5.02s/it] 12%|█▏        | 357/3000 [30:36<4:10:44,  5.69s/it] 12%|█▏        | 358/3000 [30:40<3:37:22,  4.94s/it] 12%|█▏        | 359/3000 [30:46<3:58:17,  5.41s/it] 12%|█▏        | 360/3000 [30:49<3:22:44,  4.61s/it]                                                    {'loss': 1.3486, 'learning_rate': 1.76e-05, 'epoch': 0.48}
 12%|█▏        | 360/3000 [30:49<3:22:44,  4.61s/it] 12%|█▏        | 361/3000 [30:55<3:47:07,  5.16s/it] 12%|█▏        | 362/3000 [30:59<3:26:46,  4.70s/it] 12%|█▏        | 363/3000 [31:05<3:38:14,  4.97s/it] 12%|█▏        | 364/3000 [31:09<3:30:21,  4.79s/it] 12%|█▏        | 365/3000 [31:14<3:38:06,  4.97s/it] 12%|█▏        | 366/3000 [31:19<3:38:08,  4.97s/it] 12%|█▏        | 367/3000 [31:25<3:48:13,  5.20s/it] 12%|█▏        | 368/3000 [31:29<3:34:57,  4.90s/it] 12%|█▏        | 369/3000 [31:36<4:03:48,  5.56s/it] 12%|█▏        | 370/3000 [31:39<3:28:29,  4.76s/it]                                                    {'loss': 1.3264, 'learning_rate': 1.7533333333333337e-05, 'epoch': 0.49}
 12%|█▏        | 370/3000 [31:39<3:28:29,  4.76s/it] 12%|█▏        | 371/3000 [31:46<3:53:14,  5.32s/it] 12%|█▏        | 372/3000 [31:49<3:22:40,  4.63s/it] 12%|█▏        | 373/3000 [31:54<3:35:13,  4.92s/it] 12%|█▏        | 374/3000 [31:59<3:24:47,  4.68s/it] 12%|█▎        | 375/3000 [32:04<3:38:31,  4.99s/it]
  0%|          | 0/73 [00:00<?, ?it/s][A
  3%|▎         | 2/73 [00:00<00:04, 16.58it/s][A
  5%|▌         | 4/73 [00:00<00:04, 15.77it/s][A
  8%|▊         | 6/73 [00:00<00:06, 10.90it/s][A
 11%|█         | 8/73 [00:00<00:06, 10.17it/s][A
 14%|█▎        | 10/73 [00:00<00:06,  9.06it/s][A
 15%|█▌        | 11/73 [00:01<00:07,  8.72it/s][A
 18%|█▊        | 13/73 [00:01<00:06,  9.17it/s][A
 19%|█▉        | 14/73 [00:01<00:06,  9.26it/s][A
 21%|██        | 15/73 [00:01<00:06,  8.92it/s][A
 22%|██▏       | 16/73 [00:01<00:06,  9.01it/s][A
 23%|██▎       | 17/73 [00:01<00:06,  8.97it/s][A
 25%|██▍       | 18/73 [00:01<00:06,  8.92it/s][A
 26%|██▌       | 19/73 [00:01<00:06,  8.88it/s][A
 27%|██▋       | 20/73 [00:02<00:06,  8.62it/s][A
 29%|██▉       | 21/73 [00:02<00:06,  8.60it/s][A
 32%|███▏      | 23/73 [00:02<00:05,  9.49it/s][A
 33%|███▎      | 24/73 [00:02<00:05,  9.42it/s][A
 36%|███▌      | 26/73 [00:02<00:04, 10.50it/s][A
 38%|███▊      | 28/73 [00:02<00:04, 10.41it/s][A
 41%|████      | 30/73 [00:03<00:04, 10.42it/s][A
 44%|████▍     | 32/73 [00:03<00:04,  9.93it/s][A
 47%|████▋     | 34/73 [00:03<00:03, 10.12it/s][A
 49%|████▉     | 36/73 [00:03<00:03,  9.28it/s][A
 51%|█████     | 37/73 [00:03<00:03,  9.07it/s][A
 52%|█████▏    | 38/73 [00:03<00:03,  8.99it/s][A
 53%|█████▎    | 39/73 [00:04<00:03,  8.91it/s][A
 55%|█████▍    | 40/73 [00:04<00:03,  9.15it/s][A
 56%|█████▌    | 41/73 [00:04<00:03,  9.11it/s][A
 58%|█████▊    | 42/73 [00:04<00:03,  9.32it/s][A
 60%|██████    | 44/73 [00:04<00:03,  9.61it/s][A
 62%|██████▏   | 45/73 [00:04<00:03,  8.69it/s][A
 63%|██████▎   | 46/73 [00:04<00:03,  8.46it/s][A
 64%|██████▍   | 47/73 [00:04<00:03,  8.51it/s][A
 67%|██████▋   | 49/73 [00:05<00:02,  9.26it/s][A
 70%|██████▉   | 51/73 [00:05<00:02,  9.26it/s][A
 71%|███████   | 52/73 [00:05<00:02,  9.12it/s][A
 73%|███████▎  | 53/73 [00:05<00:02,  8.96it/s][A
 74%|███████▍  | 54/73 [00:05<00:02,  8.35it/s][A
 75%|███████▌  | 55/73 [00:05<00:02,  8.54it/s][A
 77%|███████▋  | 56/73 [00:06<00:02,  8.31it/s][A
 78%|███████▊  | 57/73 [00:06<00:01,  8.35it/s][A
 79%|███████▉  | 58/73 [00:06<00:01,  8.17it/s][A
 81%|████████  | 59/73 [00:06<00:01,  8.40it/s][A
 82%|████████▏ | 60/73 [00:06<00:01,  8.60it/s][A
 84%|████████▎ | 61/73 [00:06<00:01,  8.78it/s][A
 85%|████████▍ | 62/73 [00:06<00:01,  8.60it/s][A
 86%|████████▋ | 63/73 [00:06<00:01,  8.49it/s][A
 89%|████████▉ | 65/73 [00:07<00:00,  9.65it/s][A
 92%|█████████▏| 67/73 [00:07<00:00,  9.51it/s][A
 95%|█████████▍| 69/73 [00:07<00:00,  9.60it/s][A
 96%|█████████▌| 70/73 [00:07<00:00,  9.50it/s][A
 97%|█████████▋| 71/73 [00:07<00:00,  9.31it/s][A
100%|██████████| 73/73 [00:07<00:00,  9.90it/s][A                                                    
                                               [A{'eval_loss': 1.3466495275497437, 'eval_accuracy': 0.4246803395687777, 'eval_runtime': 11.7066, 'eval_samples_per_second': 37.159, 'eval_steps_per_second': 6.236, 'epoch': 0.5}
 12%|█▎        | 375/3000 [32:16<3:38:31,  4.99s/it]
100%|██████████| 73/73 [00:09<00:00,  9.90it/s][A
                                               [A/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:527: UserWarning: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration. This warning will be raised to an exception in v4.34.

Thrown during validation:
`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
 13%|█▎        | 376/3000 [45:49<182:57:19, 251.01s/it] 13%|█▎        | 377/3000 [45:54<129:08:46, 177.25s/it] 13%|█▎        | 378/3000 [46:01<91:50:34, 126.10s/it]  13%|█▎        | 379/3000 [46:05<65:03:40, 89.36s/it]  13%|█▎        | 380/3000 [46:11<46:57:22, 64.52s/it]                                                     {'loss': 1.3294, 'learning_rate': 1.7466666666666667e-05, 'epoch': 0.51}
 13%|█▎        | 380/3000 [46:12<46:57:22, 64.52s/it] 13%|█▎        | 381/3000 [46:15<33:36:08, 46.19s/it] 13%|█▎        | 382/3000 [46:20<24:43:48, 34.01s/it] 13%|█▎        | 383/3000 [46:26<18:26:42, 25.37s/it] 13%|█▎        | 384/3000 [46:30<13:47:54, 18.99s/it] 13%|█▎        | 385/3000 [46:36<10:58:39, 15.11s/it] 13%|█▎        | 386/3000 [46:41<8:52:33, 12.22s/it]  13%|█▎        | 387/3000 [46:46<7:13:48,  9.96s/it] 13%|█▎        | 388/3000 [46:52<6:25:55,  8.87s/it] 13%|█▎        | 389/3000 [46:55<5:06:43,  7.05s/it] 13%|█▎        | 390/3000 [47:01<4:55:21,  6.79s/it]                                                    {'loss': 1.2539, 'learning_rate': 1.7400000000000003e-05, 'epoch': 0.52}
 13%|█▎        | 390/3000 [47:01<4:55:21,  6.79s/it] 13%|█▎        | 391/3000 [47:06<4:26:34,  6.13s/it] 13%|█▎        | 392/3000 [47:12<4:27:52,  6.16s/it] 13%|█▎        | 393/3000 [47:17<4:05:46,  5.66s/it] 13%|█▎        | 394/3000 [47:23<4:10:12,  5.76s/it] 13%|█▎        | 395/3000 [47:27<3:52:13,  5.35s/it] 13%|█▎        | 396/3000 [47:33<4:01:53,  5.57s/it] 13%|█▎        | 397/3000 [47:37<3:45:02,  5.19s/it] 13%|█▎        | 398/3000 [47:43<3:57:12,  5.47s/it] 13%|█▎        | 399/3000 [47:49<3:59:32,  5.53s/it] 13%|█▎        | 400/3000 [47:55<3:58:27,  5.50s/it]                                                    {'loss': 1.2979, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.53}
 13%|█▎        | 400/3000 [47:55<3:58:27,  5.50s/it] 13%|█▎        | 401/3000 [48:01<4:05:58,  5.68s/it] 13%|█▎        | 402/3000 [48:06<3:56:10,  5.45s/it] 13%|█▎        | 403/3000 [48:11<3:52:55,  5.38s/it] 13%|█▎        | 404/3000 [48:18<4:12:18,  5.83s/it] 14%|█▎        | 405/3000 [48:21<3:42:17,  5.14s/it] 14%|█▎        | 406/3000 [48:28<4:07:11,  5.72s/it] 14%|█▎        | 407/3000 [48:32<3:40:51,  5.11s/it] 14%|█▎        | 408/3000 [48:39<4:05:03,  5.67s/it] 14%|█▎        | 409/3000 [48:41<3:22:37,  4.69s/it] 14%|█▎        | 410/3000 [48:49<3:54:21,  5.43s/it]                                                    {'loss': 1.3174, 'learning_rate': 1.726666666666667e-05, 'epoch': 0.55}
 14%|█▎        | 410/3000 [48:49<3:54:21,  5.43s/it] 14%|█▎        | 411/3000 [48:52<3:29:22,  4.85s/it] 14%|█▎        | 412/3000 [48:59<4:01:06,  5.59s/it] 14%|█▍        | 413/3000 [49:03<3:35:23,  5.00s/it] 14%|█▍        | 414/3000 [49:08<3:41:13,  5.13s/it] 14%|█▍        | 415/3000 [49:13<3:33:45,  4.96s/it] 14%|█▍        | 416/3000 [49:19<3:50:26,  5.35s/it] 14%|█▍        | 417/3000 [49:24<3:39:24,  5.10s/it] 14%|█▍        | 418/3000 [49:29<3:43:50,  5.20s/it] 14%|█▍        | 419/3000 [49:34<3:34:05,  4.98s/it] 14%|█▍        | 420/3000 [49:40<3:48:59,  5.33s/it]                                                    {'loss': 1.2787, 'learning_rate': 1.72e-05, 'epoch': 0.56}
 14%|█▍        | 420/3000 [49:40<3:48:59,  5.33s/it] 14%|█▍        | 421/3000 [49:45<3:42:25,  5.17s/it] 14%|█▍        | 422/3000 [49:51<3:56:47,  5.51s/it] 14%|█▍        | 423/3000 [49:55<3:40:04,  5.12s/it] 14%|█▍        | 424/3000 [50:01<3:44:02,  5.22s/it] 14%|█▍        | 425/3000 [50:05<3:32:32,  4.95s/it] 14%|█▍        | 426/3000 [50:11<3:45:56,  5.27s/it] 14%|█▍        | 427/3000 [50:15<3:33:28,  4.98s/it] 14%|█▍        | 428/3000 [50:21<3:49:05,  5.34s/it] 14%|█▍        | 429/3000 [50:25<3:20:32,  4.68s/it] 14%|█▍        | 430/3000 [50:32<3:57:49,  5.55s/it]                                                    {'loss': 1.2701, 'learning_rate': 1.7133333333333334e-05, 'epoch': 0.57}
 14%|█▍        | 430/3000 [50:32<3:57:49,  5.55s/it] 14%|█▍        | 431/3000 [50:36<3:31:29,  4.94s/it] 14%|█▍        | 432/3000 [50:42<3:54:41,  5.48s/it] 14%|█▍        | 433/3000 [50:46<3:25:36,  4.81s/it] 14%|█▍        | 434/3000 [50:51<3:38:28,  5.11s/it] 14%|█▍        | 435/3000 [50:56<3:29:14,  4.89s/it] 15%|█▍        | 436/3000 [51:02<3:40:18,  5.16s/it] 15%|█▍        | 437/3000 [51:06<3:31:21,  4.95s/it] 15%|█▍        | 438/3000 [51:12<3:45:42,  5.29s/it] 15%|█▍        | 439/3000 [51:17<3:34:40,  5.03s/it] 15%|█▍        | 440/3000 [51:23<3:50:57,  5.41s/it]                                                    {'loss': 1.2824, 'learning_rate': 1.706666666666667e-05, 'epoch': 0.59}
 15%|█▍        | 440/3000 [51:23<3:50:57,  5.41s/it] 15%|█▍        | 441/3000 [51:26<3:18:00,  4.64s/it] 15%|█▍        | 442/3000 [51:33<3:47:05,  5.33s/it] 15%|█▍        | 443/3000 [51:35<3:14:07,  4.56s/it] 15%|█▍        | 444/3000 [51:42<3:43:00,  5.23s/it] 15%|█▍        | 445/3000 [51:46<3:18:29,  4.66s/it] 15%|█▍        | 446/3000 [51:51<3:34:40,  5.04s/it] 15%|█▍        | 447/3000 [51:56<3:24:03,  4.80s/it] 15%|█▍        | 448/3000 [52:01<3:37:11,  5.11s/it] 15%|█▍        | 449/3000 [52:06<3:34:31,  5.05s/it] 15%|█▌        | 450/3000 [52:11<3:28:35,  4.91s/it]                                                    {'loss': 1.39, 'learning_rate': 1.7e-05, 'epoch': 0.6}
 15%|█▌        | 450/3000 [52:11<3:28:35,  4.91s/it]
  0%|          | 0/73 [00:00<?, ?it/s][A
  3%|▎         | 2/73 [00:00<00:04, 16.52it/s][A
  5%|▌         | 4/73 [00:00<00:04, 15.76it/s][A
  8%|▊         | 6/73 [00:00<00:06, 10.89it/s][A
 11%|█         | 8/73 [00:00<00:06, 10.17it/s][A
 14%|█▎        | 10/73 [00:00<00:06,  9.06it/s][A
 15%|█▌        | 11/73 [00:01<00:07,  8.73it/s][A
 18%|█▊        | 13/73 [00:01<00:06,  9.16it/s][A
 19%|█▉        | 14/73 [00:01<00:06,  9.26it/s][A
 21%|██        | 15/73 [00:01<00:06,  8.92it/s][A
 22%|██▏       | 16/73 [00:01<00:06,  9.02it/s][A
 23%|██▎       | 17/73 [00:01<00:06,  8.97it/s][A
 25%|██▍       | 18/73 [00:01<00:06,  8.93it/s][A
 26%|██▌       | 19/73 [00:01<00:06,  8.89it/s][A
 27%|██▋       | 20/73 [00:02<00:06,  8.61it/s][A
 29%|██▉       | 21/73 [00:02<00:06,  8.59it/s][A
 32%|███▏      | 23/73 [00:02<00:05,  9.48it/s][A
 33%|███▎      | 24/73 [00:02<00:05,  9.41it/s][A
 36%|███▌      | 26/73 [00:02<00:04, 10.49it/s][A
 38%|███▊      | 28/73 [00:02<00:04, 10.40it/s][A
 41%|████      | 30/73 [00:03<00:04, 10.41it/s][A
 44%|████▍     | 32/73 [00:03<00:04,  9.93it/s][A
 47%|████▋     | 34/73 [00:03<00:03, 10.11it/s][A
 49%|████▉     | 36/73 [00:03<00:03,  9.29it/s][A
 51%|█████     | 37/73 [00:03<00:03,  9.06it/s][A
 52%|█████▏    | 38/73 [00:03<00:03,  8.99it/s][A
 53%|█████▎    | 39/73 [00:04<00:03,  8.90it/s][A
 55%|█████▍    | 40/73 [00:04<00:03,  9.15it/s][A
 56%|█████▌    | 41/73 [00:04<00:03,  9.11it/s][A
 58%|█████▊    | 42/73 [00:04<00:03,  9.32it/s][A
 60%|██████    | 44/73 [00:04<00:03,  9.61it/s][A
 62%|██████▏   | 45/73 [00:04<00:03,  8.70it/s][A
 63%|██████▎   | 46/73 [00:04<00:03,  8.43it/s][A
 64%|██████▍   | 47/73 [00:04<00:03,  8.49it/s][A
 67%|██████▋   | 49/73 [00:05<00:02,  9.24it/s][A
 70%|██████▉   | 51/73 [00:05<00:02,  9.23it/s][A
 71%|███████   | 52/73 [00:05<00:02,  9.11it/s][A
 73%|███████▎  | 53/73 [00:05<00:02,  8.95it/s][A
 74%|███████▍  | 54/73 [00:05<00:02,  8.34it/s][A
 75%|███████▌  | 55/73 [00:05<00:02,  8.52it/s][A
 77%|███████▋  | 56/73 [00:06<00:02,  8.31it/s][A
 78%|███████▊  | 57/73 [00:06<00:01,  8.34it/s][A
 79%|███████▉  | 58/73 [00:06<00:01,  8.16it/s][A
 81%|████████  | 59/73 [00:06<00:01,  8.40it/s][A
 82%|████████▏ | 60/73 [00:06<00:01,  8.60it/s][A
 84%|████████▎ | 61/73 [00:06<00:01,  8.78it/s][A
 85%|████████▍ | 62/73 [00:06<00:01,  8.60it/s][A
 86%|████████▋ | 63/73 [00:06<00:01,  8.49it/s][A
 89%|████████▉ | 65/73 [00:07<00:00,  9.64it/s][A
 92%|█████████▏| 67/73 [00:07<00:00,  9.50it/s][A
 95%|█████████▍| 69/73 [00:07<00:00,  9.59it/s][A
 96%|█████████▌| 70/73 [00:07<00:00,  9.49it/s][A
 97%|█████████▋| 71/73 [00:07<00:00,  9.30it/s][A
100%|██████████| 73/73 [00:07<00:00,  9.89it/s][A                                                    
                                               [A{'eval_loss': 1.328867793083191, 'eval_accuracy': 0.4252663210878221, 'eval_runtime': 11.3287, 'eval_samples_per_second': 38.398, 'eval_steps_per_second': 6.444, 'epoch': 0.6}
 15%|█▌        | 450/3000 [52:22<3:28:35,  4.91s/it]
100%|██████████| 73/73 [00:08<00:00,  9.89it/s][A
                                               [A 15%|█▌        | 451/3000 [52:26<5:40:45,  8.02s/it] 15%|█▌        | 452/3000 [52:32<5:08:23,  7.26s/it] 15%|█▌        | 453/3000 [52:37<4:37:58,  6.55s/it] 15%|█▌        | 454/3000 [52:42<4:21:49,  6.17s/it] 15%|█▌        | 455/3000 [52:46<3:56:41,  5.58s/it] 15%|█▌        | 456/3000 [52:52<3:57:13,  5.59s/it] 15%|█▌        | 457/3000 [52:56<3:40:30,  5.20s/it] 15%|█▌        | 458/3000 [53:02<3:51:01,  5.45s/it] 15%|█▌        | 459/3000 [53:06<3:37:34,  5.14s/it] 15%|█▌        | 460/3000 [53:12<3:46:35,  5.35s/it]                                                    {'loss': 1.3113, 'learning_rate': 1.6933333333333336e-05, 'epoch': 0.61}
 15%|█▌        | 460/3000 [53:13<3:46:35,  5.35s/it] 15%|█▌        | 461/3000 [53:17<3:35:05,  5.08s/it] 15%|█▌        | 462/3000 [53:23<3:44:32,  5.31s/it] 15%|█▌        | 463/3000 [53:28<3:48:22,  5.40s/it] 15%|█▌        | 464/3000 [53:33<3:43:01,  5.28s/it] 16%|█▌        | 465/3000 [53:38<3:30:57,  4.99s/it] 16%|█▌        | 466/3000 [53:43<3:39:28,  5.20s/it] 16%|█▌        | 467/3000 [53:48<3:30:45,  4.99s/it] 16%|█▌        | 468/3000 [53:55<3:58:44,  5.66s/it] 16%|█▌        | 469/3000 [54:02<4:19:51,  6.16s/it] 16%|█▌        | 470/3000 [54:09<4:31:59,  6.45s/it]                                                    {'loss': 1.3284, 'learning_rate': 1.686666666666667e-05, 'epoch': 0.63}
 16%|█▌        | 470/3000 [54:09<4:31:59,  6.45s/it] 16%|█▌        | 471/3000 [54:17<4:45:25,  6.77s/it] 16%|█▌        | 472/3000 [54:22<4:23:49,  6.26s/it] 16%|█▌        | 473/3000 [54:30<4:42:12,  6.70s/it] 16%|█▌        | 474/3000 [54:36<4:31:57,  6.46s/it] 16%|█▌        | 475/3000 [54:43<4:40:48,  6.67s/it] 16%|█▌        | 476/3000 [54:49<4:29:26,  6.41s/it] 16%|█▌        | 477/3000 [54:58<5:06:31,  7.29s/it] 16%|█▌        | 478/3000 [55:02<4:29:07,  6.40s/it] 16%|█▌        | 479/3000 [55:09<4:34:55,  6.54s/it] 16%|█▌        | 480/3000 [55:15<4:31:32,  6.47s/it]                                                    {'loss': 1.3886, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.64}
 16%|█▌        | 480/3000 [55:16<4:31:32,  6.47s/it] 16%|█▌        | 481/3000 [55:23<4:40:08,  6.67s/it] 16%|█▌        | 482/3000 [55:25<3:49:32,  5.47s/it] 16%|█▌        | 483/3000 [55:33<4:18:12,  6.16s/it] 16%|█▌        | 484/3000 [55:36<3:39:24,  5.23s/it] 16%|█▌        | 485/3000 [55:44<4:12:50,  6.03s/it] 16%|█▌        | 486/3000 [55:47<3:29:45,  5.01s/it] 16%|█▌        | 487/3000 [55:55<4:07:22,  5.91s/it] 16%|█▋        | 488/3000 [55:59<3:46:48,  5.42s/it] 16%|█▋        | 489/3000 [56:06<4:08:21,  5.93s/it] 16%|█▋        | 490/3000 [56:13<4:23:03,  6.29s/it]                                                    {'loss': 1.1952, 'learning_rate': 1.6733333333333335e-05, 'epoch': 0.65}
 16%|█▋        | 490/3000 [56:13<4:23:03,  6.29s/it] 16%|█▋        | 491/3000 [56:21<4:37:54,  6.65s/it] 16%|█▋        | 492/3000 [56:28<4:41:42,  6.74s/it] 16%|█▋        | 493/3000 [56:34<4:37:20,  6.64s/it] 16%|█▋        | 494/3000 [56:41<4:42:42,  6.77s/it] 16%|█▋        | 495/3000 [56:48<4:42:19,  6.76s/it] 17%|█▋        | 496/3000 [56:55<4:47:08,  6.88s/it] 17%|█▋        | 497/3000 [57:01<4:38:04,  6.67s/it] 17%|█▋        | 498/3000 [57:08<4:43:10,  6.79s/it] 17%|█▋        | 499/3000 [57:15<4:37:22,  6.65s/it] 17%|█▋        | 500/3000 [57:22<4:47:44,  6.91s/it]                                                    {'loss': 1.3732, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.67}
 17%|█▋        | 500/3000 [57:22<4:47:44,  6.91s/it] 17%|█▋        | 501/3000 [57:28<4:38:34,  6.69s/it] 17%|█▋        | 502/3000 [57:36<4:53:20,  7.05s/it] 17%|█▋        | 503/3000 [57:42<4:44:26,  6.83s/it] 17%|█▋        | 504/3000 [57:50<4:47:37,  6.91s/it] 17%|█▋        | 505/3000 [57:56<4:42:50,  6.80s/it] 17%|█▋        | 506/3000 [58:02<4:35:08,  6.62s/it] 17%|█▋        | 507/3000 [58:09<4:32:26,  6.56s/it] 17%|█▋        | 508/3000 [58:16<4:36:27,  6.66s/it] 17%|█▋        | 509/3000 [58:21<4:19:07,  6.24s/it] 17%|█▋        | 510/3000 [58:28<4:27:10,  6.44s/it]                                                    {'loss': 1.3444, 'learning_rate': 1.66e-05, 'epoch': 0.68}
 17%|█▋        | 510/3000 [58:28<4:27:10,  6.44s/it] 17%|█▋        | 511/3000 [58:34<4:28:51,  6.48s/it] 17%|█▋        | 512/3000 [58:42<4:38:50,  6.72s/it] 17%|█▋        | 513/3000 [58:48<4:32:49,  6.58s/it] 17%|█▋        | 514/3000 [58:55<4:40:56,  6.78s/it] 17%|█▋        | 515/3000 [59:00<4:18:07,  6.23s/it] 17%|█▋        | 516/3000 [59:09<4:48:24,  6.97s/it] 17%|█▋        | 517/3000 [59:12<3:57:26,  5.74s/it] 17%|█▋        | 518/3000 [59:20<4:32:16,  6.58s/it] 17%|█▋        | 519/3000 [59:24<4:01:38,  5.84s/it] 17%|█▋        | 520/3000 [59:31<4:12:35,  6.11s/it]                                                    {'loss': 1.3749, 'learning_rate': 1.6533333333333333e-05, 'epoch': 0.69}
 17%|█▋        | 520/3000 [59:32<4:12:35,  6.11s/it] 17%|█▋        | 521/3000 [59:38<4:29:33,  6.52s/it] 17%|█▋        | 522/3000 [59:45<4:24:13,  6.40s/it] 17%|█▋        | 523/3000 [59:52<4:33:21,  6.62s/it] 17%|█▋        | 524/3000 [59:58<4:31:44,  6.59s/it] 18%|█▊        | 525/3000 [1:00:05<4:38:07,  6.74s/it]
  0%|          | 0/73 [00:00<?, ?it/s][A
  3%|▎         | 2/73 [00:00<00:04, 16.07it/s][A
  5%|▌         | 4/73 [00:00<00:04, 15.59it/s][A
  8%|▊         | 6/73 [00:00<00:06, 10.85it/s][A
 11%|█         | 8/73 [00:00<00:06, 10.14it/s][A
 14%|█▎        | 10/73 [00:00<00:06,  9.04it/s][A
 15%|█▌        | 11/73 [00:01<00:07,  8.71it/s][A
 18%|█▊        | 13/73 [00:01<00:06,  9.15it/s][A
 19%|█▉        | 14/73 [00:01<00:06,  9.26it/s][A
 21%|██        | 15/73 [00:01<00:10,  5.76it/s][A
 22%|██▏       | 16/73 [00:01<00:08,  6.36it/s][A
 23%|██▎       | 17/73 [00:02<00:11,  4.68it/s][A
 25%|██▍       | 18/73 [00:02<00:10,  5.36it/s][A
 26%|██▌       | 19/73 [00:02<00:08,  6.01it/s][A
 27%|██▋       | 20/73 [00:02<00:08,  6.48it/s][A
 29%|██▉       | 21/73 [00:02<00:07,  6.97it/s][A
 32%|███▏      | 23/73 [00:02<00:06,  8.31it/s][A
 33%|███▎      | 24/73 [00:03<00:05,  8.51it/s][A
 36%|███▌      | 26/73 [00:03<00:04,  9.81it/s][A
 38%|███▊      | 28/73 [00:03<00:04,  9.97it/s][A
 41%|████      | 30/73 [00:03<00:04, 10.12it/s][A
 44%|████▍     | 32/73 [00:03<00:04,  9.76it/s][A
 47%|████▋     | 34/73 [00:04<00:03,  9.99it/s][A
 49%|████▉     | 36/73 [00:04<00:04,  8.32it/s][A
 51%|█████     | 37/73 [00:04<00:04,  8.30it/s][A
 52%|█████▏    | 38/73 [00:04<00:04,  8.39it/s][A
 53%|█████▎    | 39/73 [00:04<00:04,  8.44it/s][A
 55%|█████▍    | 40/73 [00:04<00:03,  8.77it/s][A
 56%|█████▌    | 41/73 [00:04<00:03,  8.84it/s][A
 58%|█████▊    | 42/73 [00:05<00:03,  9.11it/s][A
 60%|██████    | 44/73 [00:05<00:03,  9.48it/s][A
 62%|██████▏   | 45/73 [00:05<00:03,  8.63it/s][A
 63%|██████▎   | 46/73 [00:05<00:03,  8.41it/s][A
 64%|██████▍   | 47/73 [00:05<00:03,  8.47it/s][A
 67%|██████▋   | 49/73 [00:05<00:02,  9.23it/s][A
 70%|██████▉   | 51/73 [00:06<00:02,  9.24it/s][A
 71%|███████   | 52/73 [00:06<00:02,  9.11it/s][A
 73%|███████▎  | 53/73 [00:06<00:02,  8.95it/s][A
 74%|███████▍  | 54/73 [00:06<00:02,  8.35it/s][A
 75%|███████▌  | 55/73 [00:06<00:02,  8.54it/s][A
 77%|███████▋  | 56/73 [00:06<00:02,  8.32it/s][A
 78%|███████▊  | 57/73 [00:06<00:01,  8.35it/s][A
 79%|███████▉  | 58/73 [00:06<00:01,  8.18it/s][A
 81%|████████  | 59/73 [00:06<00:01,  8.41it/s][A
 82%|████████▏ | 60/73 [00:07<00:01,  8.61it/s][A
 84%|████████▎ | 61/73 [00:07<00:01,  8.79it/s][A
 85%|████████▍ | 62/73 [00:07<00:01,  8.61it/s][A
 86%|████████▋ | 63/73 [00:07<00:01,  8.50it/s][A
 89%|████████▉ | 65/73 [00:07<00:00,  9.65it/s][A
 92%|█████████▏| 67/73 [00:07<00:00,  9.51it/s][A
 95%|█████████▍| 69/73 [00:08<00:00,  9.60it/s][A
 96%|█████████▌| 70/73 [00:08<00:00,  9.51it/s][A
 97%|█████████▋| 71/73 [00:08<00:00,  9.32it/s][A
100%|██████████| 73/73 [00:08<00:00,  9.91it/s][A                                                      
                                               [A{'eval_loss': 1.3141587972640991, 'eval_accuracy': 0.425374502291338, 'eval_runtime': 13.6696, 'eval_samples_per_second': 31.823, 'eval_steps_per_second': 5.34, 'epoch': 0.7}
 18%|█▊        | 525/3000 [1:00:19<4:38:07,  6.74s/it]
100%|██████████| 73/73 [00:10<00:00,  9.91it/s][A
                                               [A 18%|█▊        | 526/3000 [1:00:23<6:53:43, 10.03s/it] 18%|█▊        | 527/3000 [1:00:30<6:17:48,  9.17s/it] 18%|█▊        | 528/3000 [1:00:37<5:49:02,  8.47s/it] 18%|█▊        | 529/3000 [1:00:44<5:35:50,  8.15s/it] 18%|█▊        | 530/3000 [1:00:50<5:09:08,  7.51s/it]                                                      {'loss': 1.3362, 'learning_rate': 1.646666666666667e-05, 'epoch': 0.71}
 18%|█▊        | 530/3000 [1:00:51<5:09:08,  7.51s/it] 18%|█▊        | 531/3000 [1:00:58<5:03:13,  7.37s/it] 18%|█▊        | 532/3000 [1:01:04<4:51:30,  7.09s/it] 18%|█▊        | 533/3000 [1:01:11<4:46:31,  6.97s/it] 18%|█▊        | 534/3000 [1:01:14<4:04:35,  5.95s/it] 18%|█▊        | 535/3000 [1:01:22<4:30:32,  6.59s/it] 18%|█▊        | 536/3000 [1:01:26<3:59:12,  5.82s/it] 18%|█▊        | 537/3000 [1:01:34<4:25:21,  6.46s/it] 18%|█▊        | 538/3000 [1:01:39<3:58:59,  5.82s/it] 18%|█▊        | 539/3000 [1:01:46<4:14:54,  6.21s/it] 18%|█▊        | 540/3000 [1:01:52<4:14:35,  6.21s/it]                                                      {'loss': 1.3381, 'learning_rate': 1.64e-05, 'epoch': 0.72}
 18%|█▊        | 540/3000 [1:01:52<4:14:35,  6.21s/it] 18%|█▊        | 541/3000 [1:02:00<4:38:08,  6.79s/it] 18%|█▊        | 542/3000 [1:02:03<3:48:26,  5.58s/it] 18%|█▊        | 543/3000 [1:02:11<4:18:33,  6.31s/it] 18%|█▊        | 544/3000 [1:02:15<3:47:04,  5.55s/it] 18%|█▊        | 545/3000 [1:02:22<4:03:57,  5.96s/it] 18%|█▊        | 546/3000 [1:02:26<3:40:01,  5.38s/it] 18%|█▊        | 547/3000 [1:02:34<4:11:34,  6.15s/it] 18%|█▊        | 548/3000 [1:02:39<4:06:48,  6.04s/it] 18%|█▊        | 549/3000 [1:02:47<4:20:58,  6.39s/it] 18%|█▊        | 550/3000 [1:02:52<4:14:00,  6.22s/it]                                                      {'loss': 1.2288, 'learning_rate': 1.6333333333333335e-05, 'epoch': 0.73}
 18%|█▊        | 550/3000 [1:02:53<4:14:00,  6.22s/it] 18%|█▊        | 551/3000 [1:03:00<4:27:36,  6.56s/it] 18%|█▊        | 552/3000 [1:03:06<4:20:12,  6.38s/it] 18%|█▊        | 553/3000 [1:03:14<4:48:26,  7.07s/it] 18%|█▊        | 554/3000 [1:03:17<3:55:18,  5.77s/it] 18%|█▊        | 555/3000 [1:03:25<4:17:35,  6.32s/it] 19%|█▊        | 556/3000 [1:03:27<3:33:45,  5.25s/it] 19%|█▊        | 557/3000 [1:03:34<3:53:52,  5.74s/it] 19%|█▊        | 558/3000 [1:03:37<3:17:40,  4.86s/it] 19%|█▊        | 559/3000 [1:03:45<4:00:08,  5.90s/it] 19%|█▊        | 560/3000 [1:03:48<3:21:16,  4.95s/it]                                                      {'loss': 1.2075, 'learning_rate': 1.6266666666666668e-05, 'epoch': 0.75}
 19%|█▊        | 560/3000 [1:03:48<3:21:16,  4.95s/it] 19%|█▊        | 561/3000 [1:03:57<4:11:06,  6.18s/it] 19%|█▊        | 562/3000 [1:04:02<3:49:47,  5.66s/it] 19%|█▉        | 563/3000 [1:04:09<4:06:56,  6.08s/it] 19%|█▉        | 564/3000 [1:04:15<4:09:36,  6.15s/it] 19%|█▉        | 565/3000 [1:04:22<4:24:13,  6.51s/it] 19%|█▉        | 566/3000 [1:04:29<4:20:17,  6.42s/it] 19%|█▉        | 567/3000 [1:04:37<4:43:33,  6.99s/it] 19%|█▉        | 568/3000 [1:04:43<4:27:57,  6.61s/it] 19%|█▉        | 569/3000 [1:04:51<4:43:20,  6.99s/it] 19%|█▉        | 570/3000 [1:04:57<4:33:25,  6.75s/it]                                                      {'loss': 1.2936, 'learning_rate': 1.62e-05, 'epoch': 0.76}
 19%|█▉        | 570/3000 [1:04:57<4:33:25,  6.75s/it] 19%|█▉        | 571/3000 [1:05:04<4:40:40,  6.93s/it] 19%|█▉        | 572/3000 [1:05:10<4:27:17,  6.61s/it] 19%|█▉        | 573/3000 [1:05:18<4:42:32,  6.98s/it] 19%|█▉        | 574/3000 [1:05:23<4:21:43,  6.47s/it] 19%|█▉        | 575/3000 [1:05:31<4:36:23,  6.84s/it] 19%|█▉        | 576/3000 [1:05:37<4:28:36,  6.65s/it] 19%|█▉        | 577/3000 [1:05:44<4:37:00,  6.86s/it] 19%|█▉        | 578/3000 [1:05:51<4:29:35,  6.68s/it] 19%|█▉        | 579/3000 [1:05:58<4:38:47,  6.91s/it] 19%|█▉        | 580/3000 [1:06:03<4:19:48,  6.44s/it]                                                      {'loss': 1.3204, 'learning_rate': 1.6133333333333334e-05, 'epoch': 0.77}
 19%|█▉        | 580/3000 [1:06:04<4:19:48,  6.44s/it] 19%|█▉        | 581/3000 [1:06:12<4:44:42,  7.06s/it] 19%|█▉        | 582/3000 [1:06:17<4:21:00,  6.48s/it] 19%|█▉        | 583/3000 [1:06:24<4:30:06,  6.71s/it] 19%|█▉        | 584/3000 [1:06:30<4:20:36,  6.47s/it] 20%|█▉        | 585/3000 [1:06:37<4:27:26,  6.64s/it] 20%|█▉        | 586/3000 [1:06:44<4:32:35,  6.78s/it] 20%|█▉        | 587/3000 [1:06:52<4:44:11,  7.07s/it] 20%|█▉        | 588/3000 [1:06:58<4:27:28,  6.65s/it] 20%|█▉        | 589/3000 [1:07:07<4:58:49,  7.44s/it] 20%|█▉        | 590/3000 [1:07:11<4:21:08,  6.50s/it]                                                      {'loss': 1.297, 'learning_rate': 1.606666666666667e-05, 'epoch': 0.79}
 20%|█▉        | 590/3000 [1:07:12<4:21:08,  6.50s/it] 20%|█▉        | 591/3000 [1:07:20<4:43:30,  7.06s/it] 20%|█▉        | 592/3000 [1:07:27<4:50:23,  7.24s/it] 20%|█▉        | 593/3000 [1:07:33<4:27:39,  6.67s/it] 20%|█▉        | 594/3000 [1:07:40<4:39:42,  6.98s/it] 20%|█▉        | 595/3000 [1:07:47<4:30:10,  6.74s/it] 20%|█▉        | 596/3000 [1:07:54<4:34:16,  6.85s/it] 20%|█▉        | 597/3000 [1:08:00<4:22:37,  6.56s/it] 20%|█▉        | 598/3000 [1:08:09<4:53:02,  7.32s/it] 20%|█▉        | 599/3000 [1:08:11<3:57:46,  5.94s/it] 20%|██        | 600/3000 [1:08:20<4:24:25,  6.61s/it]                                                      {'loss': 1.2735, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.8}
 20%|██        | 600/3000 [1:08:20<4:24:25,  6.61s/it]
  0%|          | 0/73 [00:00<?, ?it/s][A
  3%|▎         | 2/73 [00:00<00:04, 16.61it/s][A
  5%|▌         | 4/73 [00:00<00:04, 15.80it/s][A
  8%|▊         | 6/73 [00:00<00:06, 10.92it/s][A
 11%|█         | 8/73 [00:00<00:06, 10.18it/s][A
 14%|█▎        | 10/73 [00:00<00:06,  9.06it/s][A
 15%|█▌        | 11/73 [00:01<00:07,  8.73it/s][A
 16%|█▋        | 12/73 [00:01<00:14,  4.28it/s][A
 18%|█▊        | 13/73 [00:01<00:12,  4.86it/s][A
 19%|█▉        | 14/73 [00:01<00:10,  5.57it/s][A
 21%|██        | 15/73 [00:02<00:09,  6.06it/s][A
 22%|██▏       | 16/73 [00:02<00:08,  6.71it/s][A
 23%|██▎       | 17/73 [00:02<00:07,  7.20it/s][A
 25%|██▍       | 18/73 [00:02<00:07,  7.57it/s][A
 26%|██▌       | 19/73 [00:02<00:06,  7.89it/s][A
 27%|██▋       | 20/73 [00:02<00:06,  7.93it/s][A
 29%|██▉       | 21/73 [00:02<00:06,  8.10it/s][A
 32%|███▏      | 23/73 [00:02<00:05,  9.17it/s][A
 33%|███▎      | 24/73 [00:03<00:07,  6.38it/s][A
 36%|███▌      | 26/73 [00:03<00:05,  8.01it/s][A
 37%|███▋      | 27/73 [00:03<00:05,  8.09it/s][A
 40%|███▉      | 29/73 [00:03<00:05,  8.66it/s][A
 42%|████▏     | 31/73 [00:03<00:04,  9.40it/s][A
 44%|████▍     | 32/73 [00:04<00:04,  9.51it/s][A
 47%|████▋     | 34/73 [00:04<00:03,  9.86it/s][A
 48%|████▊     | 35/73 [00:04<00:04,  9.08it/s][A
 49%|████▉     | 36/73 [00:04<00:04,  9.07it/s][A
 51%|█████     | 37/73 [00:04<00:04,  8.85it/s][A
 52%|█████▏    | 38/73 [00:04<00:03,  8.81it/s][A
 53%|█████▎    | 39/73 [00:04<00:03,  8.76it/s][A
 55%|█████▍    | 40/73 [00:04<00:03,  9.07it/s][A
 56%|█████▌    | 41/73 [00:05<00:03,  9.05it/s][A
 58%|█████▊    | 42/73 [00:05<00:03,  9.29it/s][A
 60%|██████    | 44/73 [00:05<00:03,  9.60it/s][A
 62%|██████▏   | 45/73 [00:05<00:03,  7.75it/s][A
 63%|██████▎   | 46/73 [00:05<00:03,  7.77it/s][A
 64%|██████▍   | 47/73 [00:05<00:03,  7.99it/s][A
 67%|██████▋   | 49/73 [00:05<00:02,  8.91it/s][A
 70%|██████▉   | 51/73 [00:06<00:02,  9.03it/s][A
 71%|███████   | 52/73 [00:06<00:02,  8.95it/s][A
 73%|███████▎  | 53/73 [00:06<00:02,  8.83it/s][A
 74%|███████▍  | 54/73 [00:06<00:02,  8.26it/s][A
 75%|███████▌  | 55/73 [00:06<00:02,  8.47it/s][A
 77%|███████▋  | 56/73 [00:06<00:02,  8.27it/s][A
 78%|███████▊  | 57/73 [00:06<00:01,  8.32it/s][A
 79%|███████▉  | 58/73 [00:07<00:01,  8.14it/s][A
 81%|████████  | 59/73 [00:07<00:01,  8.35it/s][A
 82%|████████▏ | 60/73 [00:07<00:01,  8.56it/s][A
 84%|████████▎ | 61/73 [00:07<00:01,  8.75it/s][A
 85%|████████▍ | 62/73 [00:07<00:01,  8.58it/s][A
 86%|████████▋ | 63/73 [00:07<00:01,  8.48it/s][A
 89%|████████▉ | 65/73 [00:07<00:00,  9.63it/s][A
 92%|█████████▏| 67/73 [00:08<00:00,  9.50it/s][A
 95%|█████████▍| 69/73 [00:08<00:00,  9.60it/s][A
 96%|█████████▌| 70/73 [00:08<00:00,  9.50it/s][A
 97%|█████████▋| 71/73 [00:08<00:00,  9.31it/s][A
100%|██████████| 73/73 [00:08<00:00,  9.90it/s][A                                                      
                                               [A{'eval_loss': 1.3040447235107422, 'eval_accuracy': 0.4255517992637668, 'eval_runtime': 9.9846, 'eval_samples_per_second': 43.567, 'eval_steps_per_second': 7.311, 'epoch': 0.8}
 20%|██        | 600/3000 [1:08:30<4:24:25,  6.61s/it]
100%|██████████| 73/73 [00:09<00:00,  9.90it/s][A
                                               [A 20%|██        | 601/3000 [1:08:35<6:09:49,  9.25s/it] 20%|██        | 602/3000 [1:08:42<5:46:51,  8.68s/it] 20%|██        | 603/3000 [1:08:51<5:49:21,  8.75s/it] 20%|██        | 604/3000 [1:08:56<4:58:00,  7.46s/it] 20%|██        | 605/3000 [1:09:03<4:58:54,  7.49s/it] 20%|██        | 606/3000 [1:09:09<4:35:30,  6.90s/it] 20%|██        | 607/3000 [1:09:17<4:50:27,  7.28s/it] 20%|██        | 608/3000 [1:09:21<4:17:01,  6.45s/it] 20%|██        | 609/3000 [1:09:29<4:25:47,  6.67s/it] 20%|██        | 610/3000 [1:09:35<4:23:32,  6.62s/it]                                                      {'loss': 1.3615, 'learning_rate': 1.5933333333333336e-05, 'epoch': 0.81}
 20%|██        | 610/3000 [1:09:35<4:23:32,  6.62s/it] 20%|██        | 611/3000 [1:09:41<4:20:19,  6.54s/it] 20%|██        | 612/3000 [1:09:48<4:20:38,  6.55s/it] 20%|██        | 613/3000 [1:09:55<4:25:02,  6.66s/it] 20%|██        | 614/3000 [1:10:01<4:16:52,  6.46s/it] 20%|██        | 615/3000 [1:10:09<4:33:39,  6.88s/it] 21%|██        | 616/3000 [1:10:15<4:21:37,  6.58s/it] 21%|██        | 617/3000 [1:10:22<4:25:58,  6.70s/it] 21%|██        | 618/3000 [1:10:26<4:03:44,  6.14s/it] 21%|██        | 619/3000 [1:10:33<4:10:50,  6.32s/it] 21%|██        | 620/3000 [1:10:39<4:03:14,  6.13s/it]                                                      {'loss': 1.3061, 'learning_rate': 1.586666666666667e-05, 'epoch': 0.83}
 21%|██        | 620/3000 [1:10:40<4:03:14,  6.13s/it] 21%|██        | 621/3000 [1:10:48<4:44:04,  7.16s/it] 21%|██        | 622/3000 [1:10:54<4:27:18,  6.74s/it] 21%|██        | 623/3000 [1:11:02<4:37:05,  6.99s/it] 21%|██        | 624/3000 [1:11:07<4:20:22,  6.58s/it] 21%|██        | 625/3000 [1:11:15<4:32:31,  6.89s/it] 21%|██        | 626/3000 [1:11:21<4:17:25,  6.51s/it] 21%|██        | 627/3000 [1:11:29<4:33:37,  6.92s/it] 21%|██        | 628/3000 [1:11:35<4:24:42,  6.70s/it] 21%|██        | 629/3000 [1:11:42<4:37:26,  7.02s/it] 21%|██        | 630/3000 [1:11:49<4:32:47,  6.91s/it]                                                      {'loss': 1.2876, 'learning_rate': 1.58e-05, 'epoch': 0.84}
 21%|██        | 630/3000 [1:11:50<4:32:47,  6.91s/it] 21%|██        | 631/3000 [1:11:57<4:43:17,  7.18s/it] 21%|██        | 632/3000 [1:12:03<4:26:04,  6.74s/it] 21%|██        | 633/3000 [1:12:10<4:34:56,  6.97s/it] 21%|██        | 634/3000 [1:12:17<4:28:30,  6.81s/it] 21%|██        | 635/3000 [1:12:24<4:41:15,  7.14s/it] 21%|██        | 636/3000 [1:12:31<4:27:57,  6.80s/it] 21%|██        | 637/3000 [1:12:39<4:44:41,  7.23s/it] 21%|██▏       | 638/3000 [1:12:45<4:32:18,  6.92s/it] 21%|██▏       | 639/3000 [1:12:52<4:34:36,  6.98s/it] 21%|██▏       | 640/3000 [1:12:58<4:21:44,  6.65s/it]                                                      {'loss': 1.3009, 'learning_rate': 1.5733333333333334e-05, 'epoch': 0.85}
 21%|██▏       | 640/3000 [1:12:59<4:21:44,  6.65s/it] 21%|██▏       | 641/3000 [1:13:05<4:27:49,  6.81s/it] 21%|██▏       | 642/3000 [1:13:11<4:17:34,  6.55s/it] 21%|██▏       | 643/3000 [1:13:20<4:46:09,  7.28s/it] 21%|██▏       | 644/3000 [1:13:22<3:46:29,  5.77s/it] 22%|██▏       | 645/3000 [1:13:30<4:10:53,  6.39s/it] 22%|██▏       | 646/3000 [1:13:33<3:32:39,  5.42s/it] 22%|██▏       | 647/3000 [1:13:41<3:58:17,  6.08s/it] 22%|██▏       | 648/3000 [1:13:44<3:19:51,  5.10s/it] 22%|██▏       | 649/3000 [1:13:53<4:12:12,  6.44s/it] 22%|██▏       | 650/3000 [1:13:58<3:46:10,  5.77s/it]                                                      {'loss': 1.3322, 'learning_rate': 1.5666666666666667e-05, 'epoch': 0.87}
 22%|██▏       | 650/3000 [1:13:59<3:46:10,  5.77s/it] 22%|██▏       | 651/3000 [1:14:07<4:29:04,  6.87s/it] 22%|██▏       | 652/3000 [1:14:10<3:41:06,  5.65s/it] 22%|██▏       | 653/3000 [1:14:18<4:15:30,  6.53s/it] 22%|██▏       | 654/3000 [1:14:21<3:24:47,  5.24s/it] 22%|██▏       | 655/3000 [1:14:29<3:57:38,  6.08s/it] 22%|██▏       | 656/3000 [1:14:32<3:20:29,  5.13s/it] 22%|██▏       | 657/3000 [1:14:40<3:58:27,  6.11s/it] 22%|██▏       | 658/3000 [1:14:43<3:21:34,  5.16s/it] 22%|██▏       | 659/3000 [1:14:50<3:41:32,  5.68s/it] 22%|██▏       | 660/3000 [1:14:52<3:04:01,  4.72s/it]                                                      {'loss': 1.265, 'learning_rate': 1.5600000000000003e-05, 'epoch': 0.88}
 22%|██▏       | 660/3000 [1:14:52<3:04:01,  4.72s/it] 22%|██▏       | 661/3000 [1:15:00<3:38:50,  5.61s/it] 22%|██▏       | 662/3000 [1:15:03<3:03:28,  4.71s/it] 22%|██▏       | 663/3000 [1:15:12<3:56:13,  6.06s/it] 22%|██▏       | 664/3000 [1:15:16<3:30:21,  5.40s/it] 22%|██▏       | 665/3000 [1:15:23<3:57:14,  6.10s/it] 22%|██▏       | 666/3000 [1:15:26<3:16:16,  5.05s/it] 22%|██▏       | 667/3000 [1:15:34<3:51:40,  5.96s/it] 22%|██▏       | 668/3000 [1:15:37<3:15:02,  5.02s/it] 22%|██▏       | 669/3000 [1:15:45<3:46:57,  5.84s/it] 22%|██▏       | 670/3000 [1:15:49<3:30:19,  5.42s/it]                                                      {'loss': 1.2904, 'learning_rate': 1.5533333333333333e-05, 'epoch': 0.89}
 22%|██▏       | 670/3000 [1:15:50<3:30:19,  5.42s/it] 22%|██▏       | 671/3000 [1:15:56<3:47:18,  5.86s/it] 22%|██▏       | 672/3000 [1:16:02<3:47:10,  5.85s/it] 22%|██▏       | 673/3000 [1:16:10<4:15:23,  6.59s/it] 22%|██▏       | 674/3000 [1:16:16<4:09:21,  6.43s/it] 22%|██▎       | 675/3000 [1:16:23<4:17:32,  6.65s/it]
  0%|          | 0/73 [00:00<?, ?it/s][A
  3%|▎         | 2/73 [00:00<00:11,  6.10it/s][A
  5%|▌         | 4/73 [00:00<00:07,  9.39it/s][A
  8%|▊         | 6/73 [00:00<00:07,  8.65it/s][A
 10%|▉         | 7/73 [00:00<00:07,  8.79it/s][A
 11%|█         | 8/73 [00:00<00:07,  8.86it/s][A
 12%|█▏        | 9/73 [00:01<00:07,  8.17it/s][A
 14%|█▎        | 10/73 [00:01<00:07,  8.28it/s][A
 15%|█▌        | 11/73 [00:01<00:07,  8.08it/s][A
 18%|█▊        | 13/73 [00:01<00:06,  8.81it/s][A
 19%|█▉        | 14/73 [00:01<00:06,  8.97it/s][A
 21%|██        | 15/73 [00:02<00:10,  5.42it/s][A
 22%|██▏       | 16/73 [00:02<00:09,  6.09it/s][A
 23%|██▎       | 17/73 [00:02<00:10,  5.50it/s][A
 25%|██▍       | 18/73 [00:02<00:08,  6.14it/s][A
 26%|██▌       | 19/73 [00:02<00:08,  6.70it/s][A
 27%|██▋       | 20/73 [00:02<00:07,  7.02it/s][A
 29%|██▉       | 21/73 [00:02<00:07,  7.38it/s][A
 32%|███▏      | 23/73 [00:03<00:05,  8.61it/s][A
 33%|███▎      | 24/73 [00:03<00:05,  8.73it/s][A
 36%|███▌      | 26/73 [00:03<00:04,  9.94it/s][A
 37%|███▋      | 27/73 [00:03<00:04,  9.53it/s][A
 40%|███▉      | 29/73 [00:03<00:04,  9.57it/s][A
 42%|████▏     | 31/73 [00:03<00:04, 10.04it/s][A
 44%|████▍     | 32/73 [00:03<00:04,  9.99it/s][A
 47%|████▋     | 34/73 [00:04<00:03, 10.16it/s][A
 49%|████▉     | 36/73 [00:04<00:04,  9.20it/s][A
 51%|█████     | 37/73 [00:04<00:04,  8.96it/s][A
 52%|█████▏    | 38/73 [00:04<00:03,  8.88it/s][A
 53%|█████▎    | 39/73 [00:04<00:03,  8.79it/s][A
 55%|█████▍    | 40/73 [00:04<00:03,  9.04it/s][A
 56%|█████▌    | 41/73 [00:04<00:03,  9.00it/s][A
 58%|█████▊    | 42/73 [00:05<00:03,  9.21it/s][A
 60%|██████    | 44/73 [00:05<00:03,  9.47it/s][A
 62%|██████▏   | 45/73 [00:05<00:03,  8.59it/s][A
 63%|██████▎   | 46/73 [00:05<00:03,  8.35it/s][A
 64%|██████▍   | 47/73 [00:05<00:03,  8.41it/s][A
 67%|██████▋   | 49/73 [00:05<00:02,  9.14it/s][A
 70%|██████▉   | 51/73 [00:06<00:02,  9.14it/s][A
 71%|███████   | 52/73 [00:06<00:02,  9.01it/s][A
 73%|███████▎  | 53/73 [00:06<00:02,  8.85it/s][A
 74%|███████▍  | 54/73 [00:06<00:02,  8.27it/s][A
 75%|███████▌  | 55/73 [00:06<00:02,  8.44it/s][A
 77%|███████▋  | 56/73 [00:06<00:02,  8.22it/s][A
 78%|███████▊  | 57/73 [00:06<00:01,  8.25it/s][A
 79%|███████▉  | 58/73 [00:06<00:01,  8.06it/s][A
 81%|████████  | 59/73 [00:07<00:01,  8.29it/s][A
 82%|████████▏ | 60/73 [00:07<00:01,  8.49it/s][A
 84%|████████▎ | 61/73 [00:07<00:01,  8.66it/s][A
 85%|████████▍ | 62/73 [00:07<00:01,  8.48it/s][A
 86%|████████▋ | 63/73 [00:07<00:01,  8.38it/s][A
 89%|████████▉ | 65/73 [00:07<00:00,  9.51it/s][A
 90%|█████████ | 66/73 [00:07<00:00,  9.61it/s][A
 92%|█████████▏| 67/73 [00:07<00:00,  9.32it/s][A
 95%|█████████▍| 69/73 [00:08<00:00,  9.46it/s][A
 96%|█████████▌| 70/73 [00:08<00:00,  9.36it/s][A
 97%|█████████▋| 71/73 [00:08<00:00,  9.16it/s][A
100%|██████████| 73/73 [00:08<00:00,  9.81it/s][A                                                      
                                               [A{'eval_loss': 1.2907432317733765, 'eval_accuracy': 0.42599954924498534, 'eval_runtime': 12.7959, 'eval_samples_per_second': 33.995, 'eval_steps_per_second': 5.705, 'epoch': 0.9}
 22%|██▎       | 675/3000 [1:16:36<4:17:32,  6.65s/it]
100%|██████████| 73/73 [00:10<00:00,  9.81it/s][A
                                               [A 23%|██▎       | 676/3000 [1:16:41<6:25:23,  9.95s/it] 23%|██▎       | 677/3000 [1:16:48<5:52:49,  9.11s/it] 23%|██▎       | 678/3000 [1:16:54<5:10:43,  8.03s/it] 23%|██▎       | 679/3000 [1:17:01<5:01:24,  7.79s/it] 23%|██▎       | 680/3000 [1:17:06<4:33:48,  7.08s/it]                                                      {'loss': 1.1845, 'learning_rate': 1.546666666666667e-05, 'epoch': 0.91}
 23%|██▎       | 680/3000 [1:17:06<4:33:48,  7.08s/it] 23%|██▎       | 681/3000 [1:17:13<4:26:29,  6.90s/it] 23%|██▎       | 682/3000 [1:17:19<4:15:45,  6.62s/it] 23%|██▎       | 683/3000 [1:17:27<4:38:21,  7.21s/it] 23%|██▎       | 684/3000 [1:17:33<4:19:36,  6.73s/it] 23%|██▎       | 685/3000 [1:17:41<4:34:20,  7.11s/it] 23%|██▎       | 686/3000 [1:17:47<4:23:42,  6.84s/it] 23%|██▎       | 687/3000 [1:17:55<4:41:18,  7.30s/it] 23%|██▎       | 688/3000 [1:18:00<4:09:05,  6.46s/it] 23%|██▎       | 689/3000 [1:18:06<4:09:25,  6.48s/it] 23%|██▎       | 690/3000 [1:18:12<4:01:33,  6.27s/it]                                                      {'loss': 1.267, 'learning_rate': 1.54e-05, 'epoch': 0.92}
 23%|██▎       | 690/3000 [1:18:13<4:01:33,  6.27s/it] 23%|██▎       | 691/3000 [1:18:20<4:20:26,  6.77s/it] 23%|██▎       | 692/3000 [1:18:26<4:11:20,  6.53s/it] 23%|██▎       | 693/3000 [1:18:33<4:10:05,  6.50s/it] 23%|██▎       | 694/3000 [1:18:40<4:19:20,  6.75s/it] 23%|██▎       | 695/3000 [1:18:46<4:07:39,  6.45s/it] 23%|██▎       | 696/3000 [1:18:53<4:21:33,  6.81s/it] 23%|██▎       | 697/3000 [1:19:00<4:18:17,  6.73s/it] 23%|██▎       | 698/3000 [1:19:07<4:25:23,  6.92s/it] 23%|██▎       | 699/3000 [1:19:14<4:18:40,  6.75s/it] 23%|██▎       | 700/3000 [1:19:21<4:22:12,  6.84s/it]                                                      {'loss': 1.2653, 'learning_rate': 1.5333333333333334e-05, 'epoch': 0.93}
 23%|██▎       | 700/3000 [1:19:21<4:22:12,  6.84s/it] 23%|██▎       | 701/3000 [1:19:27<4:20:51,  6.81s/it] 23%|██▎       | 702/3000 [1:19:34<4:21:51,  6.84s/it] 23%|██▎       | 703/3000 [1:19:42<4:27:49,  7.00s/it] 23%|██▎       | 704/3000 [1:19:49<4:37:30,  7.25s/it] 24%|██▎       | 705/3000 [1:19:55<4:21:12,  6.83s/it] 24%|██▎       | 706/3000 [1:20:03<4:31:07,  7.09s/it] 24%|██▎       | 707/3000 [1:20:09<4:13:17,  6.63s/it] 24%|██▎       | 708/3000 [1:20:17<4:30:23,  7.08s/it] 24%|██▎       | 709/3000 [1:20:23<4:18:43,  6.78s/it] 24%|██▎       | 710/3000 [1:20:30<4:23:47,  6.91s/it]                                                      {'loss': 1.193, 'learning_rate': 1.5266666666666667e-05, 'epoch': 0.95}
 24%|██▎       | 710/3000 [1:20:30<4:23:47,  6.91s/it] 24%|██▎       | 711/3000 [1:20:36<4:12:23,  6.62s/it] 24%|██▎       | 712/3000 [1:20:43<4:18:17,  6.77s/it] 24%|██▍       | 713/3000 [1:20:49<4:06:57,  6.48s/it] 24%|██▍       | 714/3000 [1:20:58<4:32:52,  7.16s/it] 24%|██▍       | 715/3000 [1:21:03<4:07:54,  6.51s/it] 24%|██▍       | 716/3000 [1:21:10<4:14:24,  6.68s/it] 24%|██▍       | 717/3000 [1:21:16<4:09:22,  6.55s/it] 24%|██▍       | 718/3000 [1:21:23<4:14:36,  6.69s/it] 24%|██▍       | 719/3000 [1:21:29<4:09:56,  6.57s/it] 24%|██▍       | 720/3000 [1:21:37<4:19:07,  6.82s/it]                                                      {'loss': 1.2874, 'learning_rate': 1.5200000000000002e-05, 'epoch': 0.96}
 24%|██▍       | 720/3000 [1:21:37<4:19:07,  6.82s/it] 24%|██▍       | 721/3000 [1:21:43<4:10:33,  6.60s/it] 24%|██▍       | 722/3000 [1:21:50<4:16:33,  6.76s/it] 24%|██▍       | 723/3000 [1:21:55<4:02:31,  6.39s/it] 24%|██▍       | 724/3000 [1:22:02<4:10:33,  6.61s/it] 24%|██▍       | 725/3000 [1:22:10<4:21:53,  6.91s/it] 24%|██▍       | 726/3000 [1:22:17<4:21:06,  6.89s/it] 24%|██▍       | 727/3000 [1:22:23<4:12:10,  6.66s/it] 24%|██▍       | 728/3000 [1:22:29<4:06:16,  6.50s/it] 24%|██▍       | 729/3000 [1:22:35<3:53:28,  6.17s/it] 24%|██▍       | 730/3000 [1:22:42<4:10:59,  6.63s/it]                                                      {'loss': 1.2373, 'learning_rate': 1.5133333333333335e-05, 'epoch': 0.97}
 24%|██▍       | 730/3000 [1:22:42<4:10:59,  6.63s/it] 24%|██▍       | 731/3000 [1:22:48<4:03:41,  6.44s/it] 24%|██▍       | 732/3000 [1:22:56<4:15:03,  6.75s/it] 24%|██▍       | 733/3000 [1:23:03<4:22:17,  6.94s/it] 24%|██▍       | 734/3000 [1:23:09<4:07:39,  6.56s/it] 24%|██▍       | 735/3000 [1:23:15<4:05:08,  6.49s/it] 25%|██▍       | 736/3000 [1:23:22<4:13:10,  6.71s/it] 25%|██▍       | 737/3000 [1:23:28<4:03:18,  6.45s/it] 25%|██▍       | 738/3000 [1:23:37<4:33:43,  7.26s/it] 25%|██▍       | 739/3000 [1:23:41<3:56:57,  6.29s/it] 25%|██▍       | 740/3000 [1:23:49<4:14:58,  6.77s/it]                                                      {'loss': 1.2173, 'learning_rate': 1.5066666666666668e-05, 'epoch': 0.99}
 25%|██▍       | 740/3000 [1:23:49<4:14:58,  6.77s/it] 25%|██▍       | 741/3000 [1:23:55<4:05:04,  6.51s/it] 25%|██▍       | 742/3000 [1:24:03<4:22:55,  6.99s/it] 25%|██▍       | 743/3000 [1:24:06<3:36:03,  5.74s/it] 25%|██▍       | 744/3000 [1:24:14<4:01:23,  6.42s/it] 25%|██▍       | 745/3000 [1:24:17<3:19:55,  5.32s/it] 25%|██▍       | 746/3000 [1:24:25<3:46:57,  6.04s/it] 25%|██▍       | 747/3000 [1:24:27<3:10:30,  5.07s/it] 25%|██▍       | 748/3000 [1:24:36<3:47:56,  6.07s/it] 25%|██▍       | 749/3000 [1:24:40<3:27:44,  5.54s/it] 25%|██▌       | 750/3000 [1:24:47<3:47:58,  6.08s/it]                                                      {'loss': 1.2937, 'learning_rate': 1.5000000000000002e-05, 'epoch': 1.0}
 25%|██▌       | 750/3000 [1:24:48<3:47:58,  6.08s/it]
  0%|          | 0/73 [00:00<?, ?it/s][A
  3%|▎         | 2/73 [00:00<00:04, 15.65it/s][A
  5%|▌         | 4/73 [00:00<00:04, 15.42it/s][A
  8%|▊         | 6/73 [00:00<00:06, 10.80it/s][A
 11%|█         | 8/73 [00:00<00:06, 10.12it/s][A
 14%|█▎        | 10/73 [00:00<00:06,  9.03it/s][A
 15%|█▌        | 11/73 [00:01<00:07,  8.70it/s][A
 18%|█▊        | 13/73 [00:01<00:06,  9.14it/s][A
 19%|█▉        | 14/73 [00:01<00:06,  9.25it/s][A
 21%|██        | 15/73 [00:01<00:08,  6.50it/s][A
 22%|██▏       | 16/73 [00:01<00:08,  7.02it/s][A
 23%|██▎       | 17/73 [00:02<00:09,  6.20it/s][A
 25%|██▍       | 18/73 [00:02<00:08,  6.74it/s][A
 26%|██▌       | 19/73 [00:02<00:07,  7.21it/s][A
 27%|██▋       | 20/73 [00:02<00:07,  7.42it/s][A
 29%|██▉       | 21/73 [00:02<00:06,  7.72it/s][A
 32%|███▏      | 23/73 [00:02<00:05,  8.87it/s][A
 33%|███▎      | 24/73 [00:02<00:05,  8.94it/s][A
 36%|███▌      | 26/73 [00:02<00:04, 10.15it/s][A
 38%|███▊      | 28/73 [00:03<00:04, 10.18it/s][A
 41%|████      | 30/73 [00:03<00:04, 10.27it/s][A
 44%|████▍     | 32/73 [00:03<00:04,  9.84it/s][A
 47%|████▋     | 34/73 [00:03<00:03, 10.04it/s][A
 49%|████▉     | 36/73 [00:04<00:03,  9.25it/s][A
 51%|█████     | 37/73 [00:04<00:03,  9.04it/s][A
 52%|█████▏    | 38/73 [00:04<00:03,  8.97it/s][A
 53%|█████▎    | 39/73 [00:04<00:03,  8.89it/s][A
 55%|█████▍    | 40/73 [00:04<00:03,  9.14it/s][A
 56%|█████▌    | 41/73 [00:04<00:03,  9.11it/s][A
 58%|█████▊    | 42/73 [00:04<00:03,  9.32it/s][A
 60%|██████    | 44/73 [00:04<00:03,  9.60it/s][A
 62%|██████▏   | 45/73 [00:05<00:03,  8.70it/s][A
 63%|██████▎   | 46/73 [00:05<00:03,  8.46it/s][A
 64%|██████▍   | 47/73 [00:05<00:03,  8.52it/s][A
 67%|██████▋   | 49/73 [00:05<00:02,  9.26it/s][A
 70%|██████▉   | 51/73 [00:05<00:02,  9.24it/s][A
 71%|███████   | 52/73 [00:05<00:02,  9.11it/s][A
 73%|███████▎  | 53/73 [00:05<00:02,  8.95it/s][A
 74%|███████▍  | 54/73 [00:06<00:02,  8.35it/s][A
 75%|███████▌  | 55/73 [00:06<00:02,  8.53it/s][A
 77%|███████▋  | 56/73 [00:06<00:02,  8.31it/s][A
 78%|███████▊  | 57/73 [00:06<00:01,  8.35it/s][A
 79%|███████▉  | 58/73 [00:06<00:01,  8.17it/s][A
 81%|████████  | 59/73 [00:06<00:01,  8.41it/s][A
 82%|████████▏ | 60/73 [00:06<00:01,  8.60it/s][A
 84%|████████▎ | 61/73 [00:06<00:01,  8.79it/s][A
 85%|████████▍ | 62/73 [00:07<00:01,  8.60it/s][A
 86%|████████▋ | 63/73 [00:07<00:01,  8.49it/s][A
 89%|████████▉ | 65/73 [00:07<00:00,  9.63it/s][A
 92%|█████████▏| 67/73 [00:07<00:00,  9.50it/s][A
 95%|█████████▍| 69/73 [00:07<00:00,  9.55it/s][A
 96%|█████████▌| 70/73 [00:07<00:00,  9.46it/s][A
 97%|█████████▋| 71/73 [00:07<00:00,  9.28it/s][A
100%|██████████| 73/73 [00:08<00:00,  9.88it/s][A                                                      
                                               [A{'eval_loss': 1.279897689819336, 'eval_accuracy': 0.4262609871534821, 'eval_runtime': 11.4615, 'eval_samples_per_second': 37.953, 'eval_steps_per_second': 6.369, 'epoch': 1.0}
 25%|██▌       | 750/3000 [1:24:59<3:47:58,  6.08s/it]
100%|██████████| 73/73 [00:09<00:00,  9.88it/s][A
                                               [A/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:527: UserWarning: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration. This warning will be raised to an exception in v4.34.

Thrown during validation:
`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
Traceback (most recent call last):
  File "train_sft.py", line 219, in <module>
    main()
  File "train_sft.py", line 170, in main
    trainer.train()
  File "/data5/haoyun.xu/study/MI/MMMI/src/MI/experiment_setup/train_neuron/trainer_add_grad_mask_load_neuron_dict.py", line 1540, in train
    return inner_training_loop(
  File "/data5/haoyun.xu/study/MI/MMMI/src/MI/experiment_setup/train_neuron/trainer_add_grad_mask_load_neuron_dict.py", line 1835, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/data5/haoyun.xu/study/MI/MMMI/src/MI/experiment_setup/train_neuron/trainer_add_grad_mask_load_neuron_dict.py", line 2766, in training_step
    self.accelerator.backward(loss)
  File "/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/accelerate/accelerator.py", line 1923, in backward
    loss.backward(**kwargs)
  File "/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.55 GiB. GPU 2 has a total capacty of 79.33 GiB of which 12.41 GiB is free. Including non-PyTorch memory, this process has 66.91 GiB memory in use. Of the allocated memory 50.77 GiB is allocated by PyTorch, and 14.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2023-12-06 09:48:41,147] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2704250 closing signal SIGTERM
[2023-12-06 09:48:41,148] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2704251 closing signal SIGTERM
[2023-12-06 09:48:44,281] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 2 (pid: 2704252) of binary: /data5/haoyun.xu/miniconda3/envs/haoyunx/bin/python
Traceback (most recent call last):
  File "/data5/haoyun.xu/miniconda3/envs/haoyunx/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data5/haoyun.xu/miniconda3/envs/haoyunx/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_sft.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-12-06_09:48:41
  host      : gbox11
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 2704252)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
